{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>注：此处是文档第126页</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 微调 Torchvision 模型\n",
    "在本教程中，我们将深入探讨如何对 torchvision 模型进行微调和特征提取，所有这些模型都已经预先在1000类的magenet数据集上训练完成。本教程将深入介绍如何使用几个现代的CNN架构，并将直观展示如何微调任意的PyTorch模型。由于每个模型架构是有差异的，因此没有可以在所有场景中使用的微调代码样板。然而，研究人员必须查看现有架构并对每个模型进行自定义调整。\n",
    "\n",
    "在本文档中，我们将执行两种类型的转移学习：**微调和特征提取**。\n",
    "\n",
    "在**微调**中，我们从预训练模型开始，更新我们新任务的所有模型参数，实质上是重新训练整个模型。\n",
    "\n",
    "在**特征提取**中，我们从预训练模型开始，仅更新从中导出预测的最终图层权重。它被称为特征提取，因为我们使用预训练的CNN作为固定的特征提取器，并且仅改变输出层。\n",
    "\n",
    "有关迁移学习的更多技术信息，请参阅[此处](https://cs231n.github.io/transfer-learning/)和[这里](https://ruder.io/transfer-learning/)。\n",
    "\n",
    "通常，这两种迁移学习方法都遵循以下几个步骤：\n",
    "- 初始化预训练模型\n",
    "- 重组最后一层，使其具有与新数据集类别数相同的输出数\n",
    "- 为优化算法定义我们想要在训练期间更新的参数\n",
    "- 运行训练步骤\n",
    "\n",
    "### 1.导入相关包并打印版本号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.6.0\n",
      "Torchvision Version:  0.7.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.输入\n",
    "以下为运行时需要更改的所有参数。我们将使用的数据集 `hymenoptera_data` 可在[此处](https://download.pytorch.org/tutorial/hymenoptera_data.zip)下载。该数据集包含两类：**蜜蜂**和**蚂蚁**，其结构使得我们可以使用 `ImageFolder` 数据集，不需要编写我们自己的自定义数据集。下载数据并设置 `data_dir` 为数据集的根目录。 `model_name` 是您要使用的模型名称，必须从此列表中选择：\n",
    "\n",
    "`[resnet, alexnet, vgg, squeezenet, densenet, inception]`\n",
    "\n",
    "其他输入如下： `num_classes` 为数据集的类别数， `batch_size` 是训练的 `batch` 大小，可以根据您机器的计算能力进行调整， `num_epochsis` 是 我们想要运行的训练 `epoch` 数，`feature_extractis` 是定义我们选择微调还是特征提取的布尔值。如果 `feature_extract = False` ，将微调模型，并更新所有模型参数。如果 `feature_extract = True`，则仅更新最后一层的参数，其他参数保持不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顶级数据目录。 这里我们假设目录的格式符合ImageFolder结构\n",
    "data_dir = \"../../data/hymenoptera_data\"\n",
    "# 从[resnet, alexnet, vgg, squeezenet, densenet, inception]中选择模型\n",
    "model_name = \"squeezent\"\n",
    "# 数据集中类别数量\n",
    "num_classes = 2\n",
    "# 训练的批量大小（根据您的内存量而变化）\n",
    "batch_size = 8\n",
    "# 你要训练的epoch数\n",
    "num_epochs = 15\n",
    "\n",
    "# 用于特征提取的标志。 当为False时，我们微调整个模型，\n",
    "# 当True时我们只更新重新形成的图层参数\n",
    "feature_extract = True\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.辅助函数\n",
    "在编写调整模型的代码之前，我们先定义一些辅助函数。\n",
    "#### 3.1 模型训练和验证代码 \n",
    "- `train_model` 函数处理给定模型的训练和验证。作为输入，它需要PyTorch模型、数据加载器字典、损失函数、优化器、用于训练和验证`epoch`数，以及当模型是初始模型时的布尔标志。\n",
    "- `is_inception` 标志用于容纳 `Inception v3` 模型，因为该体系结构使用辅助输出，并且整体模型损失涉及辅助输出和最终输出，如此处所述。这个函数训练指定数量的`epoch`,并且在每个`epoch`之后运行完整的验证步骤。它还跟踪最佳性能的模型（从验证准确率方面），并在训练结束时返回性能最好的模型。在每个`epoch`之后，打印训练和验证正确率。 \n",
    "\n",
    "<font color=\"red\">epoch - n. [地质] 世；新纪元；新时代；时间上的一点(在这里指代训练时的一个时刻，训练的某一次迭代)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    \"\"\"\n",
    "    param: model: 模型\n",
    "    param: dataloaders: 数据\n",
    "    param: criterion: 标准\n",
    "    param: optimizer: 优化器\n",
    "    param: num_epochs: default=25, 迭代次数\n",
    "    param: is_inception: default=False, 初始化\n",
    "    \"\"\"\n",
    "    since = time.time()\n",
    "    val_acc_history = []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # 每个epoch都有一个训练和验证阶段\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # 迭代数据\n",
    "            for inputs, lables in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                lables = lables.to(device)\n",
    "                \n",
    "                # 0参数梯度\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 前向\n",
    "                # 如果是在训练时则跟踪轨迹\n",
    "                with torch.set_grad_enabled(phase==\"train\"):\n",
    "                    # 获取模型输出并计算损失\n",
    "                    # 开始的特殊情况，因为在训练中它有一个辅助输出。\n",
    "                    # 在训练模式下，我们通过将最终输出和辅助输出相加来计算损耗\n",
    "                    # 但在测试中我们只考虑最终输出。\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimizeinception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, lables)\n",
    "                        loss2 = criterion(aux_outputs, lables)\n",
    "                        loss = loss1 + 0.4 * loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, lables)\n",
    "                        \n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                # 统计\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == lables.data)\n",
    "                \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict()) \n",
    "            if phase == \"val\":\n",
    "                val_acc_history.append(epoch_acc)\n",
    "        \n",
    "        print()\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load basr model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 设置模型参数的 `.requires_grad` 属性\n",
    "当我们进行特征提取时，此辅助函数将模型中参数的 `.requires_grad` 属性设置为`False`。默认情况下，当我们加载一个预训练模型时，所有参数都是 `.requires_grad = True` ，如果我们从头开始训练或微调，这种设置就没问题。但是，如果我们要运行特征提取并且只想为新初始化的层计算梯度，那么我们希望所有其他参数不需要梯度变化。这将在稍后更能理解。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.初始化和重塑网络\n",
    "现在来到最有趣的部分。在这里我们对每个网络进行重塑。请注意，这不是一个自动过程，并且对每个模型都是唯一的。 回想一下，CNN模型的最后一层（通常是FC层）与数据集中的输出类的数量具有相同的节点数。由于所有模型都已在 `Imagenet` 上预先训练， 因此它们都具有大小为1000的输出层，每个类一个节点。这里的目标是将最后一层重塑为与之前具有相同数量的输入，并且具有与数据集中的类别数相同的输出数。在以下部分中，我们将讨论如何更改每个模型的体系结构。但首先，有一个关于微调和特征提取之间差异的重要细节。\n",
    "\n",
    "当进行特征提取时，我们只想更新最后一层的参数，换句话说，我们只想更新我们正在重塑层的参数。因此，我们不需要计算不需要改变的参数的梯度，因此为了提高效率，我们将其它层的 `.requires_grad` 属性设置为False。这很重要，因为默认情况下，此属性设置为True。 然后，当我们初始化新层时，默认情况下新参数 `.requires_grad = True` ，因此只更新新层的参数。当我们进行微调时，我们可以将所有 `.required_grad` 设置为默认值True。\n",
    "\n",
    "最后，请注意 `inception_v3` 的输入大小为（299,299），而所有其他模型都输入为（224,224）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
