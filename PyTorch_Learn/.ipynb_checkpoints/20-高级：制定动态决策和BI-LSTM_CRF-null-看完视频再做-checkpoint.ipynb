{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>注：此处是文档第218页</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 高级：制定动态决策和BI-LSTM CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.动态与静态深度学习工具包\n",
    "Pytorch是一种**动态**神经网络套件。另一个动态套件的例子是[Dynet](https://github.com/clab/dynet)（我之所以提到这一点，因为与Pytorch和Dynet一起使用是相似的。如果你在Dynet中看到一个例子，它可能会帮助你在Pytorch中实现它）。相反的是**静态**工具包，其中包括Theano，Keras，TensorFlow等。核心区别如下： \n",
    "* 在静态工具包中，您可以定义一次计算图，对其进行编译，然后将实例流式传输给它。 \n",
    "* 在动态工具包中，为每个实例定义计算图。它永远不会被编译并且是即时执行的。\n",
    "\n",
    "在没有很多经验的情况下，很难理解其中的差异。一个例子是假设我们想要构建一个深层组成解析器。假设我们的模型大致涉及以下步骤： \n",
    "* 我们自底向上地建造树 \n",
    "* 标记根节点（句子的单词）\n",
    "* 从那里，使用神经网络和单词的嵌入来找到形成组成部分的组合。\n",
    "\n",
    "每当你形成一个新的成分时，使用某种技术来嵌入成分。在这种情况下，我们的网络架构将完全取决于输入句子。在“The green cat scratched the wall”一句中，在模型中的某个点上，我们想要结合跨度$(i,j,r)=(1.3,NP)$（即，NP 组成部分跨越单词1到单词3，在这种情况下是“The green cat”）。\n",
    "\n",
    "然而，另一句话可能是“Somewhere, the big fat cat scratched the wall”。在这句话中，我们希望在某个时刻形成组成$(2,4,NP)$。我们想要形成的成分将取决于实例。如果我们只编译计算图一次，就像在静态工具包中那样，但编写这个逻辑将非常困难或者说是不可能的。但是，在动态工具包中，不仅有1个预定义的计算图。每个实例都可以有一个新的计算图，所以这个问题就消失了。\n",
    "\n",
    "动态工具包还具有易于调试和代码更接近宿主语言的优点（我的意思是Pytorch和Dynet看起来更像是比Keras或Theano更实际的Python代码）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Bi-LSTM条件随机场讨论\n",
    "对于本节，我们将看到用于命名实体识别的Bi-LSTM条件随机场的完整复杂示例。虽然上面的LSTM标记符通常足以用于词性标注，但是像CRF这样的序列模型对于NER上的强大性能非常重要。CRF，虽然这个名字听起来很可怕，但所有模型都是CRF，在LSTM中提供了这些功能。CRF是一个高级模型，比本教程中的任何早期模型复杂得多。如果你想跳过它，也可以。要查看您是否准备好，请查看是否可以：\n",
    "+ 在步骤i中为标记k写出维特比变量的递归。\n",
    "+ 修改上述重复以计算转发变量。\n",
    "+ 再次修改上面的重复计算以计算日志空间中的转发变量（提示：log-sum-exp）\n",
    "\n",
    "如果你可以做这三件事，你应该能够理解下面的代码。回想一下，CRF计算条件概率。设$y$为标签序列，$x$为字的输入序列。然后我们计算\n",
    "$$p(y|x)=\\frac{exp(Score(x,y))}{\\sum_{y^\\prime}exp(Score(x,y^\\prime))}$$\n",
    "通过定义一些对数电位$log\\psi_i(x,y)$来确定得分:\n",
    "$$Score(x,y)=\\sum_{i}log\\psi_i(x,y)$$\n",
    "为了使分区功能易于处理，电位必须仅查看局部特征。\n",
    "\n",
    "在Bi-LSTM CRF中，我们定义了两种潜力：发射和过渡。索引$i$处的单词的发射电位来自时间步长$i$处的Bi-LSTM的隐藏状态。转换分数存储在$T$矩阵$P$中，其中$T$是标记集。在我们的实现中，$P_{j_{y}k}$是从标签$k$转换到标签$j$的分数。所以：\n",
    "$$Score(x,y)=\\sum_{i}log\\psi_{EMIT}(y_i\\rightarrow x_i)+log\\psi_{TRANS}(y_{i-1}\\rightarrow y_i)=\\sum_{i}h_i[y_i]+P_{y_i},y_i-1$$\n",
    "在第二个表达式中，我们将标记视为分配了唯一的非负索引。\n",
    "\n",
    "如果上面的讨论过于简短，你可以查看[这个](http://www.cs.columbia.edu/~mcollins/crf.pdf)，是迈克尔柯林斯写的关于CRF的文章。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.实现说明\n",
    "下面的示例实现了日志空间中的前向算法来计算分区函数，以及用于解码的维特比算法。反向传播将自动为我们计算梯度。我们不需要手工做任何事情。\n",
    "\n",
    "这个实现并未优化。如果您了解发生了什么，您可能会很快发现在前向算法中迭代下一个标记可能是在一个大的操作中完成的。我想编码更具可读性。 如果您想进行相关更改，可以将此标记器用于实际任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2c98b87e2d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 辅助函数\n",
    "辅助函数的功能是使代码更具可读性。\n",
    "\n",
    "公式描述并不准确\n",
    "$$x_{argmax}+log(\\sum e^{x-x_{argmax}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(vec: torch.Tensor):\n",
    "    \"\"\"vec.size=(1, N)\"\"\"\n",
    "    # 将argmax作为python int返回\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    \"\"\"序列化句子\n",
    "    param: to_ix: 单词-标签对照表\n",
    "    \"\"\"\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "    \n",
    "# 以正向算法的数值稳定方式计算log sum exp\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "        \n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True)\n",
    "        \n",
    "        # 将LSTM的输出映射到标记空间\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "        \n",
    "        # 转换参数矩阵，输入i，j的得分从j转换到i\n",
    "        self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n",
    "        \n",
    "        # 这两个语句强制执行我们从不转移到开始标记的约束\n",
    "        # 并且我们永远不会从停止标记转移\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2), \n",
    "                torch.randn(2, 1, self.hidden_dim // 2))\n",
    "    \n",
    "    def _get_lstm_features(self, sentence):\n",
    "        \"\"\"将句子向量转换成特征向量并带入LSTM模型，返回LSTM的最后一层的输出特征\"\"\"\n",
    "        # 初始化隐含层\n",
    "        self.hidden = self.init_hidden()\n",
    "        # 词向量转换成特征矩阵，维度为[len(sentence), 1, embedding_dim]\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        print(\"embeds: \", embeds.size(), sentence.size())\n",
    "        # 将特征矩阵传入LSTM\n",
    "        # lstm_out的维度[len(sentence), 1, num_directions*hidden_size]\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        # [n,1,m] --> [n,m]\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        # [n,m] --> [n, m2]\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        # 不要和with_forward_alg混淆。\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeds:  torch.Size([11, 1, 5]) torch.Size([11])\n",
      "torch.Size([11, 1, 4])\n",
      "torch.Size([11, 4])\n",
      "torch.Size([11, 4])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "EMBEDDING_DIM = 5\n",
    "HIDDEN_DIM = 4\n",
    "\n",
    "# 弥补一些训练数据\n",
    "training_data = [(\n",
    "    \"the wall street journal reported today that apple corporation made money\".split(),\n",
    "    \"B I I I O O O B I O O\".split()\n",
    "), (\n",
    "    \"georgia tech is a university in georgia\".split(),\n",
    "    \"B I O O O O B\".split()\n",
    ")]\n",
    "word_to_ix = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "            \n",
    "tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}\n",
    "\n",
    "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "# 在训练前的检测预测\n",
    "with torch.no_grad():\n",
    "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)\n",
    "    # print(precheck_sent)\n",
    "    # print(precheck_tags)\n",
    "    print(model(precheck_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(17, 5)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.word_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
