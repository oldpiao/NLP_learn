{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch与Numpy对比\n",
    "\n",
    "PyTorch在很多地方个Numpy的接口是复用的，很多功能是相同的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0913, 0.9355, 0.9102],\n",
      "        [0.3800, 0.8478, 0.4604],\n",
      "        [0.3667, 0.4171, 0.9061],\n",
      "        [0.1029, 0.0426, 0.9064],\n",
      "        [0.6750, 0.3285, 0.3629]])\n",
      "[[0.91833884 0.913163   0.99102756]\n",
      " [0.07919789 0.6147196  0.82889235]\n",
      " [0.64377746 0.15389231 0.62816807]\n",
      " [0.47814221 0.64652235 0.303715  ]\n",
      " [0.73069293 0.83152217 0.94555873]]\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.rand(5, 3)\n",
    "x2 = np.random.rand(5, 3)\n",
    "print(x1)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0469e-38, 4.2246e-39, 1.0286e-38],\n",
      "        [1.0653e-38, 1.0194e-38, 8.4490e-39],\n",
      "        [1.0469e-38, 9.3674e-39, 9.9184e-39],\n",
      "        [8.7245e-39, 9.2755e-39, 8.9082e-39],\n",
      "        [9.9184e-39, 8.4490e-39, 9.6429e-39]])\n",
      "[[0.91833884 0.913163   0.99102756]\n",
      " [0.07919789 0.6147196  0.82889235]\n",
      " [0.64377746 0.15389231 0.62816807]\n",
      " [0.47814221 0.64652235 0.303715  ]\n",
      " [0.73069293 0.83152217 0.94555873]]\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.empty(5, 3)\n",
    "x2 = np.empty((5, 3))\n",
    "print(x1)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.zeros(5, 3, dtype=torch.long)\n",
    "x2 = np.zeros((5, 3), dtype=np.long)\n",
    "print(x1)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n",
      "[5.5 3. ]\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.tensor([5.5, 3])\n",
    "x2 = np.array([5.5, 3])\n",
    "print(x1)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "tensor([[0.2374, 0.6779, 0.2316],\n",
      "        [0.7260, 0.8093, 0.2615],\n",
      "        [0.2295, 0.3577, 0.8680],\n",
      "        [0.4687, 0.2119, 0.4552],\n",
      "        [0.3283, 0.2783, 0.8893]])\n"
     ]
    }
   ],
   "source": [
    "x1 = x1.new_ones(5, 3, dtype=torch.double)\n",
    "x2 = np.ones((5, 3), dtype=np.double)\n",
    "print(x1)\n",
    "print(x2)\n",
    "x3 = torch.rand_like(x1, dtype=torch.float)\n",
    "# result has the same size\n",
    "print(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x5 = np.empty((5, 3))\n",
    "x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "(5, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x3.size())\n",
    "print(x2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6120, 0.4401, 0.9214],\n",
      "        [0.0387, 0.4960, 0.4559],\n",
      "        [0.7128, 0.4231, 0.1993],\n",
      "        [0.3120, 0.5744, 0.4083],\n",
      "        [0.2299, 0.6872, 0.8828]])\n",
      "[[0.60820976 0.83108388 0.77839656]\n",
      " [0.02727458 0.80132513 0.94958339]\n",
      " [0.54657253 0.27713932 0.72403662]\n",
      " [0.04785622 0.63456246 0.56191522]\n",
      " [0.49853702 0.43188646 0.25147813]]\n",
      "tensor([[1.6120, 1.4401, 1.9214],\n",
      "        [1.0387, 1.4960, 1.4559],\n",
      "        [1.7128, 1.4231, 1.1993],\n",
      "        [1.3120, 1.5744, 1.4083],\n",
      "        [1.2299, 1.6872, 1.8828]], dtype=torch.float64)\n",
      "[[1.60820976 1.83108388 1.77839656]\n",
      " [1.02727458 1.80132513 1.94958339]\n",
      " [1.54657253 1.27713932 1.72403662]\n",
      " [1.04785622 1.63456246 1.56191522]\n",
      " [1.49853702 1.43188646 1.25147813]]\n"
     ]
    }
   ],
   "source": [
    "y1 = torch.rand(5, 3)\n",
    "y2 = np.random.random((5, 3))\n",
    "print(y1)\n",
    "print(y2)\n",
    "print(x1 + y1)\n",
    "print(x2 + y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.6120, 1.4401, 1.9214],\n",
      "        [1.0387, 1.4960, 1.4559],\n",
      "        [1.7128, 1.4231, 1.1993],\n",
      "        [1.3120, 1.5744, 1.4083],\n",
      "        [1.2299, 1.6872, 1.8828]], dtype=torch.float64)\n",
      "[[1.60820976 1.83108388 1.77839656]\n",
      " [1.02727458 1.80132513 1.94958339]\n",
      " [1.54657253 1.27713932 1.72403662]\n",
      " [1.04785622 1.63456246 1.56191522]\n",
      " [1.49853702 1.43188646 1.25147813]]\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(x1, y1))\n",
    "print(np.add(x2, y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.6120, 1.4401, 1.9214],\n",
      "        [1.0387, 1.4960, 1.4559],\n",
      "        [1.7128, 1.4231, 1.1993],\n",
      "        [1.3120, 1.5744, 1.4083],\n",
      "        [1.2299, 1.6872, 1.8828]])\n",
      "[[1.60820976 1.83108388 1.77839656]\n",
      " [1.02727458 1.80132513 1.94958339]\n",
      " [1.54657253 1.27713932 1.72403662]\n",
      " [1.04785622 1.63456246 1.56191522]\n",
      " [1.49853702 1.43188646 1.25147813]]\n"
     ]
    }
   ],
   "source": [
    "result1 = torch.empty(5, 3)\n",
    "torch.add(x1, y1, out=result1)\n",
    "print(result1)\n",
    "\n",
    "result2 = np.empty((5, 3))\n",
    "np.add(x2, y2, out=result2)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.6120, 1.4401, 1.9214],\n",
      "        [1.0387, 1.4960, 1.4559],\n",
      "        [1.7128, 1.4231, 1.1993],\n",
      "        [1.3120, 1.5744, 1.4083],\n",
      "        [1.2299, 1.6872, 1.8828]])\n"
     ]
    }
   ],
   "source": [
    "print(y1.add_(x1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.4268e-321, 4.4268e-321, 4.4861e-321],\n",
      "        [4.4861e-321, 3.1818e-321, 3.1818e-321],\n",
      "        [5.3161e-321, 5.3161e-321, 5.5138e-321],\n",
      "        [5.5138e-321, 5.5731e-321, 5.5731e-321],\n",
      "        [5.5928e-321, 5.5928e-321, 5.6323e-321]], dtype=torch.float64)\n",
      "[[1.60820976 1.83108388 1.77839656]\n",
      " [1.02727458 1.80132513 1.94958339]\n",
      " [1.54657253 1.27713932 1.72403662]\n",
      " [1.04785622 1.63456246 1.56191522]\n",
      " [1.49853702 1.43188646 1.25147813]]\n"
     ]
    }
   ],
   "source": [
    "x1 = x1.new_empty(5, 3, dtype=torch.double)\n",
    "x2 = np.empty((5, 3), dtype=np.double)\n",
    "print(x1)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.6120, 1.4401, 1.9214],\n",
      "        [1.0387, 1.4960, 1.4559],\n",
      "        [1.7128, 1.4231, 1.1993],\n",
      "        [1.3120, 1.5744, 1.4083],\n",
      "        [1.2299, 1.6872, 1.8828]])\n"
     ]
    }
   ],
   "source": [
    "print(y1.add_(x1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.4268e-321, 3.1818e-321, 5.3161e-321, 5.5731e-321, 5.5928e-321],\n",
      "       dtype=torch.float64)\n",
      "[1.83108388 1.80132513 1.27713932 1.63456246 1.43188646]\n"
     ]
    }
   ],
   "source": [
    "print(x1[:, 1])\n",
    "print(x2[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n",
      "(4, 4) (16,) (2, 8)\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.randn(4, 4)\n",
    "y1 = x1.view(16)\n",
    "z1 = x1.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "x2 = np.random.randn(4, 4)\n",
    "y2 = x2.reshape(16)\n",
    "z2 = x2.reshape((-1, 8))  # 大小-1是从其他维度推断出来的\n",
    "print(x1.size(), y1.size(), z1.size())\n",
    "print(x2.shape, y2.shape, z2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0477]) 1.0477169752120972 0.7787237167358398\n",
      "[-1.05554857] -1.0555485663913946 1.0556473333766088\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.randn(1)\n",
    "y1 = torch.randn(4, 4)\n",
    "x2 = np.random.randn(1)\n",
    "y2 = np.random.randn(4, 4)\n",
    "print(x1, x1.item(), y1[0, 0].item())\n",
    "print(x2, x2[0], y2[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 任何使张量会发生变化的操作都有一个前缀 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 tensor([[1., 2., 3., 4.],\n",
      "        [5., 6., 7., 8.]])\n",
      "y1 tensor([[2., 3., 4., 1.],\n",
      "        [6., 7., 8., 5.]])\n",
      "---用y1覆盖x1------\n",
      "x1 tensor([[2., 3., 4., 1.],\n",
      "        [6., 7., 8., 5.]])\n",
      "y1 tensor([[2., 3., 4., 1.],\n",
      "        [6., 7., 8., 5.]])\n",
      "z1 tensor([[2., 3., 4., 1.],\n",
      "        [6., 7., 8., 5.]])\n",
      "---修改参数,x1,y1互不影响,z1与x1是同一参数------\n",
      "x1 tensor([[10.,  3., 30.,  1.],\n",
      "        [ 6.,  7.,  8.,  5.]])\n",
      "y1 tensor([[ 2., 20.,  4.,  1.],\n",
      "        [ 6.,  7.,  8.,  5.]])\n",
      "z1 tensor([[10.,  3., 30.,  1.],\n",
      "        [ 6.,  7.,  8.,  5.]])\n",
      "tensor([[10.,  6.],\n",
      "        [ 3.,  7.],\n",
      "        [30.,  8.],\n",
      "        [ 1.,  5.]])\n"
     ]
    }
   ],
   "source": [
    "# pytorch的\n",
    "x1 = torch.Tensor([[1,2,3,4], [5,6,7,8]])\n",
    "y1 = torch.Tensor([[2,3,4,1], [6,7,8,5]])\n",
    "print(\"x1\", x1)\n",
    "print(\"y1\", y1)\n",
    "print('---用y1覆盖x1------')\n",
    "z1 = x1.copy_(y1)\n",
    "print(\"x1\", x1)\n",
    "print(\"y1\", y1)\n",
    "print(\"z1\", z1)\n",
    "\n",
    "print('---修改参数,x1,y1互不影响,z1与x1是同一参数------')\n",
    "x1[0,0] = 10\n",
    "y1[0,1] = 20\n",
    "z1[0,2] = 30\n",
    "print(\"x1\", x1)\n",
    "print(\"y1\", y1)\n",
    "print(\"z1\", z1)\n",
    "\n",
    "x1.t_()\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch 自动微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autograd 包是 PyTorch 中所有神经网络的核心。首先让我们简要地介绍它，然后我们将会去训练我们的第一个神经网络。该 autograd 软件包为 Tensors 上的所有操作提供自动微分。它是一个由运行定义的框架，这意味着以代码运行方式定义你的后向传播，并且每次迭代都可以不同。我们从 tensor 和 gradients 来举一些例子。\n",
    "\n",
    "**1、TENSOR**\n",
    "\n",
    "torch.Tensor是包的核心类。如果将其属性 **.requires_grad** 设置为True，则会开始跟踪针对tensor的所有操作。完成计算后，您可以调用 **.backward()** 来自动计算所有梯度。该张量的梯度将累积到 **.grad** 属性中。\n",
    "\n",
    "要停止 tensor 历史记录的跟踪，您可以调用 **.detach()**，它将其与计算历史记录分离，并防止将来的计算被跟踪。\n",
    "\n",
    "要停止跟踪历史记录（和使用内存），您还可以将代码块使用 **with torch.no_grad():** 包装起来。在评估模型时，这是特别有用，因为模型在训练阶段具有requires_grad = True 的可训练参数有利于调参，但在评估阶段我们不需要梯度。\n",
    "\n",
    "还有一个类对于 autograd 实现非常重要那就是 **Function**。Tensor 和 Function 互相连接并构建一个非循环图，它保存整个完整的计算过程的历史信息。每个张量都有一个 **.grad_fn** 属性保存着创建了张量的 Function 的引用，（如果用户自己创建张量，则grad_fn 是 None）。\n",
    "\n",
    "如果你想计算导数，你可以调用 **Tensor.backward()**。如果 Tensor 是标量（即它包含一个元素数据），则不需要指定任何参数backward()，但是如果它有更多元素，则需要指定一个 **gradient** 参数来指定张量的形状。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.ones(2, 2, requires_grad=True)\n",
    "x2 = torch.ones(2, 2, requires_grad=False)\n",
    "print(x1)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "y1 = x1 + 2\n",
    "y2 = x2 + 2\n",
    "print(y1)\n",
    "print(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x000002A05F90BD48>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(y1.grad_fn)\n",
    "print(y2.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]])\n",
      "tensor(27., grad_fn=<MeanBackward0>)\n",
      "tensor(27.)\n"
     ]
    }
   ],
   "source": [
    "z1 = y1 * y1 * 3\n",
    "z2 = y2 * y2 * 3\n",
    "\n",
    "out1 = z1.mean()\n",
    "out2 = z2.mean()\n",
    "\n",
    "print(z1)\n",
    "print(z2)\n",
    "print(out1)\n",
    "print(out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".requires_grad_( … ) 会改变张量的 requires_grad 标记。输入的标记默认为 False ，如果没有提供\n",
    "相应的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x000002A05F919C88>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度：\n",
    "\n",
    "我们现在后向传播，因为输出包含了一个标量，out.backward() 等同于\n",
    "\n",
    "out.backward(torch.tensor(1.))。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印梯度 d(out)/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x1.grad)\n",
    "print(x2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x1 = torch.ones(2, 2, requires_grad=True)  \n",
    "y1 = x1 + 2  \n",
    "z1 = y1 * y1 * 3  \n",
    "out1 = z1.mean()  \n",
    "此时对应公式：$$o = \\frac{1}{4}\\sum_{i}z_i$$\n",
    "$$z_i=(3x + 2)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个雅可比向量积的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0030, -0.1568, -1.2383], grad_fn=<MulBackward0>)\n",
      "tensor([ 0.0060, -0.3136, -2.4766], grad_fn=<MulBackward0>)\n",
      "tensor([ 0.0119, -0.6272, -4.9533], grad_fn=<MulBackward0>)\n",
      "tensor([ 0.0239, -1.2543, -9.9066], grad_fn=<MulBackward0>)\n",
      "tensor([  0.0477,  -2.5087, -19.8132], grad_fn=<MulBackward0>)\n",
      "tensor([  0.0955,  -5.0174, -39.6264], grad_fn=<MulBackward0>)\n",
      "tensor([  0.1909, -10.0348, -79.2527], grad_fn=<MulBackward0>)\n",
      "tensor([   0.3818,  -20.0695, -158.5055], grad_fn=<MulBackward0>)\n",
      "tensor([   0.7637,  -40.1390, -317.0110], grad_fn=<MulBackward0>)\n",
      "tensor([   1.5273,  -80.2781, -634.0220], grad_fn=<MulBackward0>)\n",
      "tensor([    3.0546,  -160.5562, -1268.0439], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    print(y)\n",
    "    y = y * 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以通过将代码包裹在 with torch.no_grad()，来停止对从跟踪历史中 的 .requires_grad=True 的\n",
    "张量自动求导。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x**2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x**2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch 神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 神经网络\n",
    "\n",
    "神经网络可以通过 **torch.nn** 包来构建。\n",
    "\n",
    "现在对于自动梯度(**autograd**)有一些了解，神经网络是基于自动梯度 (autograd)来定义一些模型。一个 **nn.Module** 包括层和一个方法 **forward(input)** 它会返回输出(**output**)。\n",
    "\n",
    "一个典型的神经网络训练过程包括以下几点：\n",
    "1. 定义一个包含可训练参数的神经网络\n",
    "2. 迭代整个输入\n",
    "3. 通过神经网络处理输入\n",
    "4. 计算损失(loss)\n",
    "5. 反向传播梯度到神经网络的参数\n",
    "6. 更新网络的参数，典型的用一个简单的更新方法：weight = weight - learning_rate *gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (cov1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (cov2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution(1输入图像通道，6个输出通道，5x5平方卷积)\n",
    "        # kernel\n",
    "        self.cov1 = nn.Conv2d(1, 6, 5)\n",
    "        self.cov2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = W*x + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.cov1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.cov2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension(除批处理尺寸外的所有尺寸)\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你刚定义了一个前馈函数，然后反向传播函数被自动通过 **autograd** 定义了。你可以使用任何张量\n",
    "操作在前馈函数上。  \n",
    "一个模型可训练的参数可以通过调用 **net.parameters()** 返回："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们尝试随机生成一个 32x32 的输入。注意：期望的输入维度是 32x32 。为了使用这个网络在 **MNIST** 数据集上，你需要把数据集中的图片维度修改为 32x32。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0535,  0.1269, -0.0029,  0.0112,  0.0415,  0.0075, -0.0643,  0.0505,\n",
      "         -0.0972,  0.0477]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "x_input = torch.randn(1, 1, 32, 32)\n",
    "out = net(x_input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把所有参数梯度缓存器置零，用随机的梯度来反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在继续之前，让我们复习一下所有见过的类。\n",
    "- **torch.Tensor**\n",
    "    - A multi-dimensional array with support for autograd operations like backward(). Also holds the gradient w.r.t. the tensor.\n",
    "    - 支持autograd操作(如backward())的多维数组。也保持梯度w.r.t张量。\n",
    "- **nn.Module**\n",
    "    - Neural network module. Convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading,etc.\n",
    "    - 神经网络模块。方便的封装参数的方式，有帮助移动他们到GPU，导出，加载等。\n",
    "- **nn.Parameter** \n",
    "    - A kind of Tensor, that is automatically registered as a parameter when assigned as an attribute to a Module.\n",
    "    - 一种张量，当被指定为模块的属性时，自动作为参数注册。\n",
    "- **autograd.Function** \n",
    "    - Implements forward and backward definitions of an autograd operation. Every Tensor operation, creates at least a single Function node, that connects to functions thatcreated a Tensor and encodes its history.\n",
    "    - 实现autograd操作的向前和向后定义。每个张量操作，至少创建一个函数节点，连接到创建一个张量并编码其历史的函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此，我们完成了：\n",
    "1. 定义一个神经网络\n",
    "2. 处理输入以及调用反向传播\n",
    "\n",
    "还剩下：\n",
    "1. 计算损失值\n",
    "2. 更新网络中的权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  损失函数\n",
    "\n",
    "一个损失函数需要一对输入：模型输出和目标，然后计算一个值来评估输出距离目标有多远。有一些不同的损失函数在 nn 包中。一个简单的损失函数就是 **nn.MSELoss** ，这计算了均方误差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7545, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(x_input)\n",
    "target = torch.randn(10)  # a dummy target, for example(一个虚构的目标的例子)\n",
    "target = target.view(1, -1)  # make it the same shape as output（使其与输出拥有相同的shape）\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x000002A05F90FB08>\n",
      "<AddmmBackward object at 0x000002A05F90F348>\n",
      "<AccumulateGrad object at 0x000002A05F90FB08>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)\n",
    "print(loss.grad_fn.next_functions[0][0])  # linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 反向传播\n",
    "为了实现反向传播损失，我们所有需要做的事情仅仅是使用 **loss.backward()**。你**需要清空现存的梯度**，要不然帝都将会和现存的梯度累计到一起。\n",
    "\n",
    "现在我们调用 loss.backward() ，然后看一下 con1 的偏置项在反向传播之前和之后的变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0030, -0.0196,  0.0062,  0.0120, -0.0036, -0.0012])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()  # # zeroes the gradient buffers of all parameters (所有参数的梯度设置为0)\n",
    "\n",
    "print(\"conv1.bias.grad before backward\")\n",
    "print(net.cov1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(\"conv1.bias.grad after backward\")\n",
    "print(net.cov1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们看到了，如何使用损失函数。  \n",
    "唯一剩下的事情就是更新神经网络的参数。  \n",
    "更新神经网络参数：  \n",
    "最简单的更新规则就是随机梯度下降: weight = weight -learning_rate * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尽管如此，如果你是用神经网络，你想使用不同的更新规则，类似于 **SGD, Nesterov-SGD, Adam，RMSProp**, 等。为了让这可行，我们建立了一个小包：**torch.optim** 实现了所有的方法。使用它非常的简单。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "optimizer.zero_grad()  # zero the gradient buffers（梯度缓冲设为0）\n",
    "output = net(x_input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()  # Does the update(是否更新)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
