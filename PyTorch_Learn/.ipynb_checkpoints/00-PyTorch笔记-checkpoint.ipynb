{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sections\n",
    "- [markdown使用](#markdown使用)\n",
    "- [公式符号](#公式符号)\n",
    "- [PyTorch](#PyTorch)\n",
    "    - [自动求导](#自动求导)\n",
    "        - [y.data.norm()](#y.data.norm())\n",
    "        - [with torch.no_grad()](#with&nbsp;torch.no_grad())\n",
    "    - [神经网络](#神经网络)\n",
    "        - [nn.Conv2d卷积](#nn.Conv2d卷积)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### markdown使用\n",
    "- [jupyter notebook中markdown使用](https://www.jianshu.com/p/576e3f1049f8)\n",
    "- [如何在Markdown文档中插入空格?](https://www.cnblogs.com/klchang/p/10203404.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 公式符号\n",
    "更多用法：[https://blog.csdn.net/smilejiasmile/article/details/80670742](https://blog.csdn.net/smilejiasmile/article/details/80670742)\n",
    "\n",
    "J_\\alpha(x) = \\sum_{m=0}^\\infty \\frac{(-1)^m}{m! \\Gamma (m + \\alpha + 1)} {\\left({ \\frac{x}{2} }\\right)}^{2m + \\alpha} \\text {，行内公式示例}\n",
    "$$ J_\\alpha(x) = \\sum_{m=0}^\\infty \\frac{(-1)^m}{m! \\Gamma (m + \\alpha + 1)} {\\left({ \\frac{x}{2} }\\right)}^{2m + \\alpha} \\text {，行内公式示例} $$\n",
    "\\int_0^1 {x^2} \\,{\\rm d}x\n",
    "$$\\int_0^1 {x^2} \\,{\\rm d}x$$\n",
    "\\lim_{n \\to +\\infty} \\frac{1}{n(n+1)} \\quad and \\quad \\lim_{x\\leftarrow{示例}} \\frac{1}{n(n+1)}\n",
    "$$ \\lim_{n \\to +\\infty} \\frac{1}{n(n+1)} \\quad and \\quad \\lim_{x\\leftarrow{示例}} \\frac{1}{n(n+1)} $$\n",
    "\\sum_{i=1}^n \\frac{1}{i^2} \\quad and \\quad \\prod_{i=1}^n \\frac{1}{i^2} \\quad and \\quad \\bigcup_{i=1}^{2} R\n",
    "$$\\sum_{i=1}^n \\frac{1}{i^2} \\quad and \\quad \\prod_{i=1}^n \\frac{1}{i^2} \\quad and \\quad \\bigcup_{i=1}^{2} R$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前所有见过的类：\n",
    "- **torch.Tensor**\n",
    "    - A multi-dimensional array with support for autograd operations like backward(). Also holds the gradient w.r.t. the tensor.\n",
    "    - 支持autograd操作(如backward())的多维数组。也保持梯度w.r.t张量。\n",
    "- **nn.Module**\n",
    "    - Neural network module. Convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading,etc.\n",
    "    - 神经网络模块。方便的封装参数的方式，有帮助移动他们到GPU，导出，加载等。\n",
    "- **nn.Parameter** \n",
    "    - A kind of Tensor, that is automatically registered as a parameter when assigned as an attribute to a Module.\n",
    "    - 一种张量，当被指定为模块的属性时，自动作为参数注册。\n",
    "- **autograd.Function** \n",
    "    - Implements forward and backward definitions of an autograd operation. Every Tensor operation, creates at least a single Function node, that connects to functions thatcreated a Tensor and encodes its history.\n",
    "    - 实现autograd操作的向前和向后定义。每个张量操作，至少创建一个函数节点，连接到创建一个张量并编码其历史的函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y.data.norm() \n",
    "y的L2范数：$\\sum_{i=1}^ny^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.3757)\n",
      "tensor(4.3757, grad_fn=<SqrtBackward>)\n",
      "tensor([-947.1443, -188.2073,  567.7290], grad_fn=<MulBackward0>)\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x*2\n",
    "print(y.data.norm())\n",
    "print(torch.sqrt(torch.sum(torch.pow(y,2))))  #其实就是对y张量L2范数，先对y中每一项取平方，之后累加，最后取根号\n",
    "i=0\n",
    "while y.data.norm()<1000:\n",
    "  y = y*2\n",
    "  i+=1\n",
    "print(y)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with&nbsp;torch.no_grad()\n",
    "[https://www.jianshu.com/p/1cea017f5d11](https://www.jianshu.com/p/1cea017f5d11)  \n",
    "作用：使由requires_grad = True的tensor生成的新tensor的requires_grad=False，gead_fn=None,即不会对新tensor求导。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(10, 5, requires_grad = True)\n",
    "y = torch.randn(10, 5, requires_grad = True)\n",
    "z = torch.randn(10, 5, requires_grad = True)\n",
    "with torch.no_grad():\n",
    "    w = x + y + z\n",
    "    print(w.requires_grad)\n",
    "    print(w.grad_fn)\n",
    "print(w.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络\n",
    "\n",
    "神经网络可以通过 **torch.nn** 包来构建。\n",
    "\n",
    "现在对于自动梯度(**autograd**)有一些了解，神经网络是基于自动梯度 (autograd)来定义一些模型。一个 **nn.Module** 包括层和一个方法 **forward(input)** 它会返回输出(**output**)。\n",
    "\n",
    "一个典型的神经网络训练过程包括以下几点：\n",
    "1. 定义一个包含可训练参数的神经网络\n",
    "2. 迭代整个输入\n",
    "3. 通过神经网络处理输入\n",
    "4. 计算损失(loss)\n",
    "5. 反向传播梯度到神经网络的参数\n",
    "6. 更新网络的参数，典型的用一个简单的更新方法：weight = weight - learning_rate *gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Conv2d卷积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
