{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sections\n",
    "- [markdown使用](#markdown使用)\n",
    "- [公式符号](#公式符号)\n",
    "- [PyTorch](#PyTorch)\n",
    "    - [自动求导](#自动求导)\n",
    "        - [y.data.norm()](#y.data.norm())\n",
    "        - [with torch.no_grad()](#with&nbsp;torch.no_grad())\n",
    "    - [神经网络](#神经网络)\n",
    "        - [nn.Conv2d卷积](#nn.Conv2d卷积)\n",
    "        - [nn.MaxPool2d](#nn.MaxPool2d)\n",
    "        - [nn.VagPool2d](#nn.MaxPool2d)\n",
    "        - [nn.Linear](#nn.Linear)\n",
    " \n",
    " \n",
    " [back to bottom](#Bottom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### markdown使用\n",
    "- [jupyter notebook中markdown使用](https://www.jianshu.com/p/576e3f1049f8)\n",
    "- [如何在Markdown文档中插入空格?](https://www.cnblogs.com/klchang/p/10203404.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 公式符号\n",
    "更多用法：[https://blog.csdn.net/smilejiasmile/article/details/80670742](https://blog.csdn.net/smilejiasmile/article/details/80670742)\n",
    "\n",
    "J_\\alpha(x) = \\sum_{m=0}^\\infty \\frac{(-1)^m}{m! \\Gamma (m + \\alpha + 1)} {\\left({ \\frac{x}{2} }\\right)}^{2m + \\alpha} \\text {，行内公式示例}\n",
    "$$ J_\\alpha(x) = \\sum_{m=0}^\\infty \\frac{(-1)^m}{m! \\Gamma (m + \\alpha + 1)} {\\left({ \\frac{x}{2} }\\right)}^{2m + \\alpha} \\text {，行内公式示例} $$\n",
    "\\int_0^1 {x^2} \\,{\\rm d}x\n",
    "$$\\int_0^1 {x^2} \\,{\\rm d}x$$\n",
    "\\lim_{n \\to +\\infty} \\frac{1}{n(n+1)} \\quad and \\quad \\lim_{x\\leftarrow{示例}} \\frac{1}{n(n+1)}\n",
    "$$ \\lim_{n \\to +\\infty} \\frac{1}{n(n+1)} \\quad and \\quad \\lim_{x\\leftarrow{示例}} \\frac{1}{n(n+1)} $$\n",
    "\\sum_{i=1}^n \\frac{1}{i^2} \\quad and \\quad \\prod_{i=1}^n \\frac{1}{i^2} \\quad and \\quad \\bigcup_{i=1}^{2} R\n",
    "$$\\sum_{i=1}^n \\frac{1}{i^2} \\quad and \\quad \\prod_{i=1}^n \\frac{1}{i^2} \\quad and \\quad \\bigcup_{i=1}^{2} R$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前所有见过的类：\n",
    "- **torch.Tensor**\n",
    "    - A multi-dimensional array with support for autograd operations like backward(). Also holds the gradient w.r.t. the tensor.\n",
    "    - 支持autograd操作(如backward())的多维数组。也保持梯度w.r.t张量。\n",
    "- **nn.Module**\n",
    "    - Neural network module. Convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading,etc.\n",
    "    - 神经网络模块。方便的封装参数的方式，有帮助移动他们到GPU，导出，加载等。\n",
    "- **nn.Parameter** \n",
    "    - A kind of Tensor, that is automatically registered as a parameter when assigned as an attribute to a Module.\n",
    "    - 一种张量，当被指定为模块的属性时，自动作为参数注册。\n",
    "- **autograd.Function** \n",
    "    - Implements forward and backward definitions of an autograd operation. Every Tensor operation, creates at least a single Function node, that connects to functions thatcreated a Tensor and encodes its history.\n",
    "    - 实现autograd操作的向前和向后定义。每个张量操作，至少创建一个函数节点，连接到创建一个张量并编码其历史的函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y.data.norm() \n",
    "y的L2范数：$\\sum_{i=1}^ny^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.3757)\n",
      "tensor(4.3757, grad_fn=<SqrtBackward>)\n",
      "tensor([-947.1443, -188.2073,  567.7290], grad_fn=<MulBackward0>)\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x*2\n",
    "print(y.data.norm())\n",
    "print(torch.sqrt(torch.sum(torch.pow(y,2))))  #其实就是对y张量L2范数，先对y中每一项取平方，之后累加，最后取根号\n",
    "i=0\n",
    "while y.data.norm()<1000:\n",
    "  y = y*2\n",
    "  i+=1\n",
    "print(y)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with&nbsp;torch.no_grad()\n",
    "[https://www.jianshu.com/p/1cea017f5d11](https://www.jianshu.com/p/1cea017f5d11)  \n",
    "作用：使由requires_grad = True的tensor生成的新tensor的requires_grad=False，gead_fn=None,即不会对新tensor求导。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(10, 5, requires_grad = True)\n",
    "y = torch.randn(10, 5, requires_grad = True)\n",
    "z = torch.randn(10, 5, requires_grad = True)\n",
    "with torch.no_grad():\n",
    "    w = x + y + z\n",
    "    print(w.requires_grad)\n",
    "    print(w.grad_fn)\n",
    "print(w.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络\n",
    "\n",
    "神经网络可以通过 **torch.nn** 包来构建。\n",
    "\n",
    "现在对于自动梯度(**autograd**)有一些了解，神经网络是基于自动梯度 (autograd)来定义一些模型。一个 **nn.Module** 包括层和一个方法 **forward(input)** 它会返回输出(**output**)。\n",
    "\n",
    "一个典型的神经网络训练过程包括以下几点：\n",
    "1. 定义一个包含可训练参数的神经网络\n",
    "2. 迭代整个输入\n",
    "3. 通过神经网络处理输入\n",
    "4. 计算损失(loss)\n",
    "5. 反向传播梯度到神经网络的参数\n",
    "6. 更新网络的参数，典型的用一个简单的更新方法：weight = weight - learning_rate *gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Conv2d卷积"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一维卷积与二维卷积: [https://blog.csdn.net/qq_26369907/article/details/88366147](https://blog.csdn.net/qq_26369907/article/details/88366147)\n",
    "- **nn.Conv1d**: 一维的卷积能处理多维数据\n",
    "- **nn.Conv2d**: 二维卷积可以处理二维数据\n",
    "\n",
    "nn.Conv2d(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True))  \n",
    "nn.Conv1d(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True))  \n",
    "**参数**：\n",
    "- **in_channel**:　输入数据的通道数，例RGB图片通道数为3；\n",
    "- **out_channel**: 输出数据的通道数，这个根据模型调整；\n",
    "- **kennel_size**: 卷积核大小，可以是int，或tuple；kennel_size=2,意味着卷积大小(2,2)， kennel_size=（2,3），意味着卷积大小（2，3）即非正方形卷积\n",
    "- **stride**：步长，默认为1，与kennel_size类似，stride=2,意味着步长上下左右扫描皆为2， stride=（2,3），左右扫描步长为2，上下为3；\n",
    "- **padding**：零填充\n",
    "\n",
    "\n",
    "**卷积计算过程**：  \n",
    "h/w = (h/w - kennel_size + 2padding) / stride + 1  \n",
    "x = ([10,16,30,32]),其中h=30,w=32,对于卷积核长分别是 h:3，w:2 ；对于步长分别是h：2，w:1；padding默认0；  \n",
    "h = (30 - 3 + 20)/ 2 +1 = 27/2 +1 = 13+1 =14  \n",
    "w =(32 - 2 + 2*0)/ 1 +1 = 30/1 +1 = 30+1 =31  \n",
    "batch = 10, out_channel = 33  \n",
    "故： y= ([10, 33, 14, 31])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 例：nn.Conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 16, 30, 32, 34])\n",
      "Net_1D(\n",
      "  (layers): Sequential(\n",
      "    (0): Conv1d(16, 16, kernel_size=(3, 2, 2), stride=(2, 2, 1), padding=[2, 2, 2])\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n",
      "torch.Size([10, 16, 16, 18, 37])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x = torch.randn(10, 16, 30, 32, 34)\n",
    "# batch, channel , height , width\n",
    "print(x.shape)\n",
    "class Net_1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_1D, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=16, out_channels=16, kernel_size=(3, 2, 2), stride=(2, 2, 1), padding=[2,2,2]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        output = self.layers(x)\n",
    "        log_probs = F.log_softmax(output, dim=1)\n",
    "        return  log_probs\n",
    "\n",
    "n = Net_1D()  # in_channel,out_channel,kennel,\n",
    "print(n)\n",
    "y = n(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 例：nn.Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 16, 30, 32])\n",
      "Conv2d(16, 33, kernel_size=(3, 2), stride=(2, 1))\n",
      "torch.Size([10, 33, 14, 31])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.randn(10, 16, 30, 32) # batch, channel , height , width\n",
    "print(x.shape)\n",
    "m = nn.Conv2d(16, 33, (3, 2), (2,1))  # in_channel, out_channel ,kennel_size,stride\n",
    "print(m)\n",
    "y = m(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.MaxPool2d\n",
    "[https://www.jianshu.com/p/9d93a3391159](https://www.jianshu.com/p/9d93a3391159)\n",
    "\n",
    "\n",
    "class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)  \n",
    "**nn.Avg_pool**: 对于Avg_pool来说，参数和Max_pool是完全相同的，主要区别就是在kernel中取的是平均值操作。\n",
    "\n",
    "参数：\n",
    "- kernel_size(int or tuple): max pooling的窗口大小，\n",
    "- stride(int or tuple, optional): max pooling的窗口移动的步长。默认值是kernel_size\n",
    "- padding(int or tuple, optional): 输入的每一条边补充0的层数\n",
    "- dilation(int or tuple, optional): 一个控制窗口中元素步幅的参数\n",
    "- return_indices: 如果等于True，会返回输出最大值的序号，对于上采样操作会有帮助\n",
    "- ceil_mode: 如果等于True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"learn_img/16715697-3d69c563031e9ec4.webp\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "a = torch.randn(3,5,10)\n",
    "b = nn.MaxPool2d((5,1))  # kernel = 5 步长为1\n",
    "c = b(a)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pool操作并不改变张量的通道数\n",
    "x = torch.rand(1,3,7,7)\n",
    "out = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "out.forward(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1,3,7,7)\n",
    "out = nn.AvgPool2d(kernel_size=2,stride=2)\n",
    "out.forward(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Linear\n",
    "[https://blog.csdn.net/qq_42079689/article/details/102873766](https://blog.csdn.net/qq_42079689/article/details/102873766)  \n",
    "更详细：[https://blog.csdn.net/weixin_44613063/article/details/90274590](https://blog.csdn.net/weixin_44613063/article/details/90274590)\n",
    "\n",
    "PyTorch的 **nn.Linear()** 是用于设置网络中的全连接层的，需要注意的是全连接层的输入与输出都是二维张量，一般形状为\\[batch_size, size\\]\n",
    "$$y=xA^T+b$$\n",
    "\n",
    "计算步骤：\n",
    "```python\n",
    "@weak_script_method\n",
    "def forward(self, input):\n",
    "    return F.linear(input, self.weight, self.bias) \n",
    "```\n",
    "返回的是：**input * weight + bias**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12288])\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# in_features由输入张量的形状决定，out_features则决定了输出张量的形状 \n",
    "connected_layer = nn.Linear(in_features = 64*64*3, out_features = 1)\n",
    "\n",
    "# 假定输入的图像形状为[64,64,3]\n",
    "input = torch.randn(1,64,64,3)\n",
    "\n",
    "# 将四维张量转换为二维张量之后，才能作为全连接层的输入\n",
    "input = input.view(1,64*64*3)\n",
    "print(input.shape)\n",
    "output = connected_layer(input) # 调用全连接层\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bottom\n",
    "[[back to top](#Sections)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
