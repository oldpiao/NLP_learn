{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sections\n",
    "- [markdown使用](#markdown使用)\n",
    "- [公式符号](#公式符号)\n",
    "- [常见词翻译](#常见词翻译)\n",
    "- [机器学习知识](#机器学习知识)\n",
    "- [python知识点](#python知识点)\n",
    "    - [魔法方法](#魔法方法)\n",
    "        - [\\_\\_getitem\\_\\_](#\\_\\_getitem\\_\\_)\n",
    "        -[\\_\\_call\\_\\_](#\\_\\_call\\_\\_)\n",
    "    - [copy.deepcopy()](#copy.deepcopy())\n",
    "- [numpy](#numpy)\n",
    "- [pillow](#pillow)\n",
    "- [PyTorch](#PyTorch)\n",
    "    - [基础方法](#基础方法)\n",
    "        - [torch.clamp](#torch.clamp)\n",
    "        - [torch.set_grad_enabled()](#torch.set_grad_enabled())\n",
    "        - [torch.randperm()](#torch.randperm())\n",
    "    - [自动求导](#自动求导)\n",
    "        - [y.data.norm()](#y.data.norm())\n",
    "        - [with torch.no_grad()](#with&nbsp;torch.no_grad())\n",
    "    - [神经网络](#神经网络)  # torch.nn\n",
    "        - [nn.Module](#nn.Module)\n",
    "            - [model.state_dict()](#model.state_dict())\n",
    "            - [model.train()与model.eval()](#model.train()与model.eval())\n",
    "        - [nn.Conv2d卷积](#nn.Conv2d卷积)\n",
    "        - [nn.MaxPool2d](#nn.MaxPool2d)\n",
    "        - [nn.VagPool2d](#nn.MaxPool2d)\n",
    "        - [nn.Linear](#nn.Linear)\n",
    "        - [nn.ReLU&nbsp;vs&nbsp;F.ReLU](#nn.ReLU&nbsp;vs&nbsp;F.ReLU)\n",
    "        - [nn.functional as F](#nn.functional)\n",
    "    - [优化器库](#优化器库)  # torch.optim\n",
    "        - [optim.SGD()](#optim.SGD())\n",
    "        - [optim.lr_scheduler](#optim.lr_scheduler)\n",
    "        - [.step()](#.step())\n",
    "- [torchvision库](#torchvision库)\n",
    "    - [torchvision.datasets](#torchvision.datasets)\n",
    "    - [torchvision.models](#torchvision.models)\n",
    "    - [torchvision.transforms](#torchvision.transforms)\n",
    "        - [transforms.Compose()](#transforms.Compose)\n",
    "        - [transforms.ToTensor() and transforms.Normalize()](#transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to bottom](#Bottom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# markdown使用\n",
    "- [jupyter notebook中markdown使用](https://www.jianshu.com/p/576e3f1049f8)\n",
    "- [如何在Markdown文档中插入空格?](https://www.cnblogs.com/klchang/p/10203404.html)\n",
    "- [MarkDown用法详细介绍](https://blog.csdn.net/qq_27229113/article/details/73692171)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 公式符号\n",
    "更多用法：[https://blog.csdn.net/smilejiasmile/article/details/80670742](https://blog.csdn.net/smilejiasmile/article/details/80670742)\n",
    "\n",
    "J_\\alpha(x) = \\sum_{m=0}^\\infty \\frac{(-1)^m}{m! \\Gamma (m + \\alpha + 1)} {\\left({ \\frac{x}{2} }\\right)}^{2m + \\alpha} \\text {，行内公式示例}\n",
    "$$ J_\\alpha(x) = \\sum_{m=0}^\\infty \\frac{(-1)^m}{m! \\Gamma (m + \\alpha + 1)} {\\left({ \\frac{x}{2} }\\right)}^{2m + \\alpha} \\text {，行内公式示例} $$\n",
    "\\int_0^1 {x^2} \\,{\\rm d}x\n",
    "$$\\int_0^1 {x^2} \\,{\\rm d}x$$\n",
    "\\lim_{n \\to +\\infty} \\frac{1}{n(n+1)} \\quad and \\quad \\lim_{x\\leftarrow{示例}} \\frac{1}{n(n+1)}\n",
    "$$ \\lim_{n \\to +\\infty} \\frac{1}{n(n+1)} \\quad and \\quad \\lim_{x\\leftarrow{示例}} \\frac{1}{n(n+1)} $$\n",
    "\\sum_{i=1}^n \\frac{1}{i^2} \\quad and \\quad \\prod_{i=1}^n \\frac{1}{i^2} \\quad and \\quad \\bigcup_{i=1}^{2} R\n",
    "$$\\sum_{i=1}^n \\frac{1}{i^2} \\quad and \\quad \\prod_{i=1}^n \\frac{1}{i^2} \\quad and \\quad \\bigcup_{i=1}^{2} R$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 常见词翻译"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- accuracy - n.**精确度**，准确性\n",
    "- affine - adj. （数）**仿射的**; n. 姻亲\n",
    "    - 图像缩放，移动，旋转等可以用一个仿射模型表示\n",
    "- bias - n. 偏见；偏爱；斜纹；乖离率; vt. 使存偏见; adj. 偏斜的; adv. 偏斜地;\n",
    "- correct - **修正、调整（数据）**\n",
    "    - adj. （政治或思想）正确的；恰当的；端正的\n",
    "    - v. 改正；批改（学生作业）；校正；指出错误；抵消；校准（仪器）；修正、调整（数据）\n",
    "- criterion - 标准\n",
    "- device - n. 装置；策略；图案；设备；**终端**\n",
    "- dropout - 失学儿童；退学；信号丢失；辍学者\n",
    "    - **在神经网络中为避免数据量过少时造成的过拟合，在训练模型时加入的随机扰动，一般方法是，训练时随机的将一些链路上的训练结果设置为0**\n",
    "- embedding - **嵌入式**；嵌入；嵌入层；包埋\n",
    "- epoch - n. \\[地质\\] 世；新纪元；新时代；**时间上的一点**(在这里指代训练时的一个时刻，训练的某一次迭代)\n",
    "- hidden - 隐藏；隐蔽性；隐忧；不可告人\n",
    "    - **隐含层（神经网络）**\n",
    "- inception - n. **起初**；获得学位；盗梦空间；全面启动；潜行凶间；奠基\n",
    "- optim - 使最优化\n",
    "- optimizer - 最优控制器\n",
    "- validation - 确认；**验证**；校验；检验\n",
    "- stride - **步幅**；大步走；跨跃弹奏法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器学习知识\n",
    "- [【卷积神经网络】卷积层，池化层，全连接层](https://blog.csdn.net/idwtwt/article/details/87567432)\n",
    "- [【深度学习】多通道图像卷积过程及计算方式](https://blog.csdn.net/briblue/article/details/83063170)\n",
    "    - 滤波器: 使卷积过程中通道数变化的罪魁祸首\n",
    "    - [科普-深度学习中的卷积-卷积核和滤波器的区别](https://www.cnblogs.com/elitphil/p/12040671.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# python知识点\n",
    "## 魔法方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\_\\_getitem\\_\\_\n",
    "\n",
    "[python：__getitem__方法详解](https://blog.csdn.net/weixin_42557907/article/details/81589574)\n",
    "\n",
    "这个方法返回与指定键想关联的值。对序列来说，键应该是0~n-1的整数，其中n为序列的长度。对映射来说，键可以是任何类型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这个方法被调用\n",
      "This is python\n"
     ]
    }
   ],
   "source": [
    "class Tag:\n",
    "    def __init__(self):\n",
    "        self.change={'python':'This is python'}\n",
    " \n",
    "    def __getitem__(self, item):\n",
    "        print('这个方法被调用')\n",
    "        return self.change[item]\n",
    " \n",
    "a=Tag()\n",
    "print(a['python'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\_\\_call\\_\\_\n",
    "[Python __call__详解](https://www.jianshu.com/p/e1d95c4e1697?utm_source=oschina-app)\n",
    "\n",
    "``__call__``的作用相当于闭包中的内层方法，可以让一个类的实例对象当成方法调用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### copy.deepcopy()\n",
    "深度拷贝\n",
    "\n",
    "[python中copy()和deepcopy()详解](https://blog.csdn.net/u010712012/article/details/79754132)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 2, 3], [4, 2, 3]]\n",
      "[[1, 2, 3], [1, 2, 3]]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "a = [1,2,3]\n",
    "b = [a, a]\n",
    "c = b\n",
    "d = copy.deepcopy(b)\n",
    "a[0] = 4\n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### np.unique()\n",
    "该函数是去除数组中的重复数字，并进行排序之后输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n",
      "[list([1, 1, 3, 2]) list([1, 2, 4])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([2,3,4,1,1,1,2])\n",
    "b = np.array([[1,2,4],[1,1,3,2]])\n",
    "print(np.unique(a))\n",
    "print(np.unique(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### np.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 3], dtype=int64),)\n",
      "(array([1, 4], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.where(np.array([True, False, False, True])))\n",
    "print(np.where([False, True, False, False, True]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### np.transpose()\n",
    "\n",
    "多维数组的维度转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  1  2]\n",
      "  [ 3  4  5]]\n",
      "\n",
      " [[ 6  7  8]\n",
      "  [ 9 10 11]]]\n",
      "[[[ 0  1  2]\n",
      "  [ 6  7  8]]\n",
      "\n",
      " [[ 3  4  5]\n",
      "  [ 9 10 11]]]\n",
      "[[[ 0  6]\n",
      "  [ 3  9]]\n",
      "\n",
      " [[ 1  7]\n",
      "  [ 4 10]]\n",
      "\n",
      " [[ 2  8]\n",
      "  [ 5 11]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 0,1,2\n",
    "x = np.arange(12).reshape((2,2,3))\n",
    "print(x)\n",
    "# 1,0,2\n",
    "x = x.transpose(1,0,2)\n",
    "print(x)\n",
    "# 2,0,1\n",
    "x = np.transpose(x, (2,0,1))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### np.clip()\n",
    "[Numpy clip函数](https://www.jianshu.com/p/c36bdac9c4b0)\n",
    "\n",
    "clip这个函数将将数组中的元素限制在a_min, a_max之间，大于a_max的就使得它等于 a_max，小于a_min,的就使得它等于a_min。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 5, 6, 7, 8, 8])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x=np.array([1,2,3,5,6,7,8,9])\n",
    "np.clip(x,3,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pillow\n",
    "[Python图像库PIL的类Image及其方法介绍](https://blog.csdn.net/leemboy/article/details/83792729)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前所有见过的类：\n",
    "- **torch.Tensor**\n",
    "    - A multi-dimensional array with support for autograd operations like backward(). Also holds the gradient w.r.t. the tensor.\n",
    "    - 支持autograd操作(如backward())的多维数组。也保持梯度w.r.t张量。\n",
    "- **nn.Module**\n",
    "    - Neural network module. Convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading,etc.\n",
    "    - 神经网络模块。方便的封装参数的方式，有帮助移动他们到GPU，导出，加载等。\n",
    "- **nn.Parameter** \n",
    "    - A kind of Tensor, that is automatically registered as a parameter when assigned as an attribute to a Module.\n",
    "    - 一种张量，当被指定为模块的属性时，自动作为参数注册。\n",
    "- **autograd.Function** \n",
    "    - Implements forward and backward definitions of an autograd operation. Every Tensor operation, creates at least a single Function node, that connects to functions thatcreated a Tensor and encodes its history.\n",
    "    - 实现autograd操作的向前和向后定义。每个张量操作，至少创建一个函数节点，连接到创建一个张量并编码其历史的函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**目前见到的一些方法的使用，仅展示，未整理：**  \n",
    "<font color=red>注：行与行之间没有关联，一般开头的变量都是一个张量，同名变量意义大部分情况相同</font>\n",
    "```python\n",
    "dtype = torch.float\n",
    "x = torch.randn(64, 100, device=device, dtype=dtype)  # 随机数据\n",
    "w = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "h = x.mm(w1)\n",
    "h_relu = h.clamp(min=0)\n",
    "loss = (y_pred - y).pow(2).sum()\n",
    "loss.item()\n",
    "h_relu.t()\n",
    "grad_h_relu.clone()\n",
    "grad_h[h < 0] = 0\n",
    "loss.backward()\n",
    "with torch.no_grad():\n",
    "w1.grad.zero_()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**切换CPU、GPU**\n",
    "```python\n",
    "device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda: 0\") \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "```\n",
    "**加载模型**\n",
    "```python\n",
    "checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "```\n",
    "**初始化参数**\n",
    "```python\n",
    "embedding_sd = checkpoint['embedding']\n",
    "embedding.load_state_dict(embedding_sd)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.clamp\n",
    "[https://blog.csdn.net/qq_21997625/article/details/90717359](https://blog.csdn.net/qq_21997625/article/details/90717359)  \n",
    "\n",
    "**`torch.clamp(input, min, max, out=None)`**  \n",
    "将输入input张量每个元素的范围限制到区间 [min,max]，返回结果到一个新张量。\n",
    "- input (Tensor) – 输入张量\n",
    "- min (Number) – 限制范围下限\n",
    "- max (Number) – 限制范围上限\n",
    "- out (Tensor, optional) – 输出张量\n",
    "\n",
    "相当于RuLE激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2],\n",
      "        [5],\n",
      "        [8],\n",
      "        [7],\n",
      "        [4],\n",
      "        [1],\n",
      "        [2],\n",
      "        [5],\n",
      "        [5],\n",
      "        [5]])\n",
      "tensor([[3],\n",
      "        [5],\n",
      "        [8],\n",
      "        [7],\n",
      "        [4],\n",
      "        [3],\n",
      "        [3],\n",
      "        [5],\n",
      "        [5],\n",
      "        [5]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a=torch.randint(low=0,high=10,size=(10,1))\n",
    "print(a)\n",
    "a=torch.clamp(a,3,9)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.set_grad_enabled()\n",
    "[【学习笔记】关于volatile和0.4.0版本后的torch.set_grad_enabled()](https://blog.csdn.net/zzzpy/article/details/88873109)\n",
    "\n",
    "使用`torch.set_grad_enabled(True|False)`开启/关闭全部梯度，关闭后之后该张量（tensor）就是不可求导的了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.randperm()\n",
    "\n",
    "返回一个0到n-1的数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 8, 2, 5, 1, 6, 0, 7, 3, 9])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.randperm(10)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 8, 2, 5, 1, 6, 0, 7, 3, 9]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y.data.norm() \n",
    "y的L2范数：$\\sum_{i=1}^ny^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.3757)\n",
      "tensor(4.3757, grad_fn=<SqrtBackward>)\n",
      "tensor([-947.1443, -188.2073,  567.7290], grad_fn=<MulBackward0>)\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x*2\n",
    "print(y.data.norm())\n",
    "print(torch.sqrt(torch.sum(torch.pow(y,2))))  #其实就是对y张量L2范数，先对y中每一项取平方，之后累加，最后取根号\n",
    "i=0\n",
    "while y.data.norm()<1000:\n",
    "  y = y*2\n",
    "  i+=1\n",
    "print(y)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with&nbsp;torch.no_grad()\n",
    "[https://www.jianshu.com/p/1cea017f5d11](https://www.jianshu.com/p/1cea017f5d11)  \n",
    "作用：使由requires_grad = True的tensor生成的新tensor的requires_grad=False，gead_fn=None,即不会对新tensor求导。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(10, 5, requires_grad = True)\n",
    "y = torch.randn(10, 5, requires_grad = True)\n",
    "z = torch.randn(10, 5, requires_grad = True)\n",
    "with torch.no_grad():\n",
    "    w = x + y + z\n",
    "    print(w.requires_grad)\n",
    "    print(w.grad_fn)\n",
    "print(w.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络\n",
    "\n",
    "神经网络可以通过 **torch.nn** 包来构建。\n",
    "\n",
    "现在对于自动梯度(**autograd**)有一些了解，神经网络是基于自动梯度 (autograd)来定义一些模型。一个 **nn.Module** 包括层和一个方法 **forward(input)** 它会返回输出(**output**)。\n",
    "\n",
    "一个典型的神经网络训练过程包括以下几点：\n",
    "1. 定义一个包含可训练参数的神经网络\n",
    "2. 迭代整个输入\n",
    "3. 通过神经网络处理输入\n",
    "4. 计算损失(loss)\n",
    "5. 反向传播梯度到神经网络的参数\n",
    "6. 更新网络的参数，典型的用一个简单的更新方法：weight = weight - learning_rate *gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.Module\n",
    "Torch自定义模型需要通过继承该类实现\n",
    "\n",
    "[pytorch教程之nn.Module类详解——使用Module类来自定义网络层](https://blog.csdn.net/qq_27825451/article/details/90705328)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### model.state_dict()\n",
    "[【PyTorch】state_dict详解](https://blog.csdn.net/bigFatCat_Tom/article/details/90722261)\n",
    "\n",
    "在pytorch中，torch.nn.Module模块中的state_dict变量存放训练过程中需要学习的权重和偏执系数，state_dict作为python的字典对象将每一层的参数映射成tensor张量，需要注意的是torch.nn.Module模块中的state_dict只包含卷积层和全连接层的参数，当网络中存在batchnorm时，例如vgg网络结构，torch.nn.Module模块中的state_dict也会存放batchnorm's running_mean。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### model.train()与model.eval()\n",
    "\n",
    "[model.train()与model.eval()的用法](https://blog.csdn.net/Qy1997/article/details/106455717)  \n",
    "[Batch Normalization](https://www.cnblogs.com/hutao722/p/9842199.html)  \n",
    "[Dropout VS Batch Normalization? 是时候放弃Dropout了](https://www.cnblogs.com/hutao722/p/9946047.html)\n",
    "\n",
    "如果模型中有BN层(Batch Normalization）和Dropout，需要在训练时添加model.train()，在测试时添加model.eval()。其中model.train()是保证BN层用每一批数据的均值和方差，而model.eval()是保证BN用全部训练数据的均值和方差；而对于Dropout，model.train()是随机取一部分网络连接来训练更新参数，而model.eval()是利用到了所有网络连接。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.Conv2d卷积"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一维卷积与二维卷积: [https://blog.csdn.net/qq_26369907/article/details/88366147](https://blog.csdn.net/qq_26369907/article/details/88366147)\n",
    "- **nn.Conv1d** - 一维的卷积能处理多维数据\n",
    "- **nn.Conv2d** - 二维卷积可以处理二维数据\n",
    "\n",
    "nn.Conv2d(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True))  \n",
    "nn.Conv1d(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True))  \n",
    "**参数**：\n",
    "- **in_channel** - 输入数据的通道数，例RGB图片通道数为3；\n",
    "- **out_channel** - 输出数据的通道数，这个根据模型调整；\n",
    "- **kennel_size** - 卷积核大小，可以是int，或tuple；kennel_size=2,意味着卷积大小(2,2)， kennel_size=(2,3)，意味着卷积大小(2,3)即非正方形卷积\n",
    "- **stride** - 步长，默认为1，与kennel_size类似，stride=2,意味着步长上下左右扫描皆为2， stride=（2,3），左右扫描步长为2，上下为3；\n",
    "- **padding** - 零填充\n",
    "\n",
    "\n",
    "**卷积计算过程**：  \n",
    "h/w = (h/w - kennel_size + 2padding) / stride + 1  \n",
    "x = ([10,16,30,32]),其中h=30,w=32,对于卷积核长分别是 h:3，w:2 ；对于步长分别是h：2，w:1；padding默认0；  \n",
    "h = (30 - 3 + 20)/ 2 +1 = 27/2 +1 = 13+1 =14  \n",
    "w =(32 - 2 + 2*0)/ 1 +1 = 30/1 +1 = 30+1 =31  \n",
    "batch = 10, out_channel = 33  \n",
    "故： y= ([10, 33, 14, 31])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 例：nn.Conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 16, 30, 32, 34])\n",
      "Net_1D(\n",
      "  (layers): Sequential(\n",
      "    (0): Conv1d(16, 16, kernel_size=(3, 2, 2), stride=(2, 2, 1), padding=[2, 2, 2])\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n",
      "torch.Size([10, 16, 16, 18, 37])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x = torch.randn(10, 16, 30, 32, 34)\n",
    "# batch, channel , height , width\n",
    "print(x.shape)\n",
    "class Net_1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_1D, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=16, out_channels=16, kernel_size=(3, 2, 2), stride=(2, 2, 1), padding=[2,2,2]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        output = self.layers(x)\n",
    "        log_probs = F.log_softmax(output, dim=1)\n",
    "        return  log_probs\n",
    "\n",
    "n = Net_1D()  # in_channel,out_channel,kennel,\n",
    "print(n)\n",
    "y = n(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 例：nn.Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 16, 30, 32])\n",
      "Conv2d(16, 33, kernel_size=(3, 2), stride=(2, 1))\n",
      "torch.Size([10, 33, 14, 31])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.randn(10, 16, 30, 32) # batch, channel , height , width\n",
    "print(x.shape)\n",
    "m = nn.Conv2d(16, 33, (3, 2), (2,1))  # in_channel, out_channel ,kennel_size,stride\n",
    "print(m)\n",
    "y = m(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.MaxPool2d\n",
    "**池化层**\n",
    "\n",
    "[https://www.jianshu.com/p/9d93a3391159](https://www.jianshu.com/p/9d93a3391159)\n",
    "\n",
    "\n",
    "class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)  \n",
    "**nn.Avg_pool**: 对于Avg_pool来说，参数和Max_pool是完全相同的，主要区别就是在kernel中取的是平均值操作。\n",
    "\n",
    "参数：\n",
    "- kernel_size(int or tuple): max pooling的窗口大小，\n",
    "- stride(int or tuple, optional): max pooling的窗口移动的步长。默认值是kernel_size\n",
    "- padding(int or tuple, optional): 输入的每一条边补充0的层数\n",
    "- dilation(int or tuple, optional): 一个控制窗口中元素步幅的参数\n",
    "- return_indices: 如果等于True，会返回输出最大值的序号，对于上采样操作会有帮助\n",
    "- ceil_mode: 如果等于True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"learn_img/16715697-3d69c563031e9ec4.webp\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "a = torch.randn(3,5,10)\n",
    "b = nn.MaxPool2d((5,1))  # kernel = 5 步长为1\n",
    "c = b(a)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pool操作并不改变张量的通道数\n",
    "x = torch.rand(1,3,7,7)\n",
    "out = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "out.forward(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1,3,7,7)\n",
    "out = nn.AvgPool2d(kernel_size=2,stride=2)\n",
    "out.forward(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.Linear\n",
    "[https://blog.csdn.net/qq_42079689/article/details/102873766](https://blog.csdn.net/qq_42079689/article/details/102873766)  \n",
    "更详细：[https://blog.csdn.net/weixin_44613063/article/details/90274590](https://blog.csdn.net/weixin_44613063/article/details/90274590)\n",
    "\n",
    "PyTorch的 **nn.Linear()** 是用于设置网络中的全连接层的，需要注意的是全连接层的输入与输出都是二维张量，一般形状为\\[batch_size, size\\]\n",
    "$$y=xA^T+b$$\n",
    "\n",
    "计算步骤：\n",
    "```python\n",
    "@weak_script_method\n",
    "def forward(self, input):\n",
    "    return F.linear(input, self.weight, self.bias) \n",
    "```\n",
    "返回的是：**input * weight + bias**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12288])\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# in_features由输入张量的形状决定，out_features则决定了输出张量的形状 \n",
    "connected_layer = nn.Linear(in_features = 64*64*3, out_features = 1)\n",
    "\n",
    "# 假定输入的图像形状为[64,64,3]\n",
    "input = torch.randn(1,64,64,3)\n",
    "\n",
    "# 将四维张量转换为二维张量之后，才能作为全连接层的输入\n",
    "input = input.view(1,64*64*3)\n",
    "print(input.shape)\n",
    "output = connected_layer(input) # 调用全连接层\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.ReLU&nbsp;vs&nbsp;F.ReLU\n",
    "[https://blog.csdn.net/u011501388/article/details/86602275](https://blog.csdn.net/u011501388/article/details/86602275)\n",
    "\n",
    "在如下网络中，AlexNet_1与AlexNet_2实现的结果是一致的，但是可以看到将ReLU层添加到网络有两种不同的实现，即nn.ReLU和F.ReLU两种实现方法。\n",
    "其中nn.ReLU作为一个层结构，必须添加到nn.Module容器中才能使用，而F.ReLU则作为一个函数调用，看上去作为一个函数调用更方便更简洁。具体使用哪种方式，取决于编程风格。在PyTorch中,nn.X都有对应的函数版本F.X，但是并不是所有的F.X均可以用于forward或其它代码段中，因为当网络模型训练完毕时，在存储model时，在forward中的F.X函数中的参数是无法保存的。也就是说，在forward中，使用的F.X函数一般均没有状态参数，比如F.ReLU，F.avg_pool2d等，均没有参数，它们可以用在任何代码片段中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    " \n",
    "class AlexNet_1(nn.Module):\n",
    " \n",
    "    def __init__(self, num_classes=n):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "         )\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    " \n",
    "class AlexNet_2(nn.Module):\n",
    " \n",
    "    def __init__(self, num_classes=n):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "         )\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = F.ReLU(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "F.max_pool1d\n",
    "F.affine_grid\n",
    "F.grid_sample\n",
    "F.log_softmax\n",
    "F.nll_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- F.max_pool1d\n",
    "- F.affine_grid  - 仿射网络\n",
    "- [F.grid_sample](https://blog.csdn.net/Hansry/article/details/88870389)  - grid_sample其实就是实现了一个双线性插值的图片大小变换。\n",
    "- F.log_softmax - 在数学上等价于log(softmax(x))\n",
    "- F.nll_loss - 负对数似然损失\n",
    "    - [Pytorch损失函数torch.nn.NLLLoss()详解](https://blog.csdn.net/Jeremy_lf/article/details/102725285)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化器库\n",
    "[深度学习——优化器算法Optimizer详解（BGD、SGD、MBGD、Momentum、NAG、Adagrad、Adadelta、RMSprop、Adam）](https://www.cnblogs.com/guoyaohua/p/8542554.html)\n",
    "\n",
    "在机器学习、深度学习中使用的优化算法除了常见的梯度下降，还有 Adadelta，Adagrad，RMSProp 等几种优化器\n",
    "\n",
    "torch中的优化器库\n",
    "```python\n",
    "import torch.optim as optim\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.optim' from 'D:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\pytorch\\\\lib\\\\site-packages\\\\torch\\\\optim\\\\__init__.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### optim.SGD()\n",
    "[torch.optim.SGD()各参数的解释](https://www.cnblogs.com/peixu/p/13194328.html)\n",
    "\n",
    "`torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)`\n",
    "\n",
    "Nesterov动量基于On the importance of initialization and momentum in deep learning中的公式.  \n",
    "**参数：**\n",
    "- params (iterable) – 待优化参数的iterable或者是定义了参数组的dict\n",
    "- lr (float) – 学习率\n",
    "- momentum (float, 可选) – 动量因子（默认：0）\n",
    "- weight_decay (float, 可选) – 权重衰减（L2惩罚）（默认：0）\n",
    "- dampening (float, 可选) – 动量的抑制因子（默认：0）\n",
    "- nesterov (bool, 可选) – 使用Nesterov动量（默认：False）\n",
    "\n",
    "```python\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "optimizer.zero_grad()\n",
    "loss_fn(model(input), target).backward()\n",
    "optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### optim.lr_scheduler\n",
    "**设置模型步长的库，可以使用函数优化步长**  \n",
    "[pytorch中调整学习率: torch.optim.lr_scheduler](https://blog.csdn.net/Strive_For_Future/article/details/83213971)  \n",
    "\n",
    "**.StepLR()**\n",
    "`class torch.optim.lr_scheduler.StepLR(optimizer,step_size,gamma=0.1,last_epoch=-1)`  \n",
    "设置每个参数组的学习率为$lr*\\lambda^{n}$, $n=\\frac{epoch}{step\\_size}$,当last_epoch=-1时,令lr=lr  \n",
    "参数:\n",
    "- **optimizer**(Optimizer对象): 优化器\n",
    "- **step_size**(整数类型): 调整学习率的步长,每过step_size次,更新一次学习率\n",
    "- **gamma**(float 类型): 学习率下降的乘数因子\n",
    "- **last_epoch**(int类型): 最后一次epoch的索引,默认为-1.\n",
    "\n",
    "示例：\n",
    "```python\n",
    ">>> from torch.optim.lr_scheduler import StepLR\n",
    ">>> # Assuming optimizer uses lr = 0.05 for all groups\n",
    ">>> # lr = 0.05     if epoch < 30\n",
    ">>> # lr = 0.005    if 30 <= epoch < 60\n",
    ">>> # lr = 0.0005   if 60 <= epoch < 90\n",
    ">>> # ...\n",
    ">>> scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    ">>> for epoch in range(100):\n",
    ">>>     scheduler.step()\n",
    ">>>     train(...)\n",
    ">>>     validate(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .step()\n",
    "[torch之optimizer.step() 和loss.backward()和scheduler.step()的关系与区别](https://blog.csdn.net/yangwangnndd/article/details/95622893)\n",
    "\n",
    "**`optimizer.step()`** 是用来更新梯度的，将方向传播计算出的残差依据优化器更新到权重上。  \n",
    "**`scheduler.step()`** 按照Pytorch的定义是用来更新优化器的学习率的，一般是按照epoch为单位进行更换，即多少个epoch后更换一次学习率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "model = torch.nn.Linear(100, 1000)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 100, gamma = 0.1)\n",
    "# model = net.train(model, loss_function, optimizer, scheduler, num_epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torchvision库\n",
    "\n",
    "torchvision库简介（翻译）: [https://www.cnblogs.com/yjphhw/p/9773333.html](https://www.cnblogs.com/yjphhw/p/9773333.html)  \n",
    "torchvision是独立于pytorch的关于图像操作的一些方便工具库。  \n",
    "torchvision的详细介绍在：[https://pypi.org/project/torchvision/](https://pypi.org/project/torchvision/)  \n",
    "torchvision主要包括一下几个包：  \n",
    "- vision.datasets : 几个常用视觉数据集，可以下载和加载，这里主要的高级用法就是可以看源码如何自己写自己的Dataset的子类\n",
    "- vision.models : 流行的模型，例如 AlexNet, VGG, ResNet和Densenet以及 与训练好的参数。\n",
    "- vision.transforms : 常用的图像操作，例如：随机切割，旋转，数据类型转换，图像到tensor,numpy数组到tensor,tensor到图像等。\n",
    "- vision.utils : 用于把形似 (3 x H x W) 的张量保存到硬盘中，给一个mini-batch的图像可以产生一个图像格网。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torchvision.datasets\n",
    "数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torchvision.models\n",
    "[PyTorch源码解读之torchvision.models](https://blog.csdn.net/u014380165/article/details/79119664)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torchvision.transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transforms.Compose()\n",
    "**链式处理、流水线处理**，可以将多个数据操作集成在一块处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR)\n",
       "    RandomHorizontalFlip(p=0.5)\n",
       "    ToTensor()\n",
       "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224), #随机裁剪一个area然后再resize\n",
    "    transforms.RandomHorizontalFlip(), #随机水平翻转\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "data_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Compose(\n",
       "     RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR)\n",
       "     RandomHorizontalFlip(p=0.5)\n",
       "     ToTensor()\n",
       "     Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       " ),\n",
       " 'val': Compose(\n",
       "     Resize(size=224, interpolation=PIL.Image.BILINEAR)\n",
       "     CenterCrop(size=(224, 224))\n",
       "     ToTensor()\n",
       "     Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       " )}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据扩充和训练规范化\n",
    "# 只需验证标准化\n",
    "input_size = 224\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "data_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transforms.ToTensor()\n",
    "#### transforms.Normalize()\n",
    "\n",
    "**数据归一化，标准化**  \n",
    "[关于transforms.Normalize()函数](https://blog.csdn.net/jzwong/article/details/104272600)\n",
    "\n",
    "```python\n",
    "transforms.ToTensor(),  # 归一化\n",
    "transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))  # 标准化\n",
    "```\n",
    "那transform.Normalize()是怎么工作的呢？以上面代码为例，ToTensor()能够把灰度范围从0-255变换到0-1之间，而后面的transform.Normalize()则把0-1变换到(-1,1).具体地说，对每个通道而言，Normalize执行以下操作："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToTensor()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bottom\n",
    "[[back to top](#Sections)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
