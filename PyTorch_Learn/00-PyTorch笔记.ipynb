{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sections\n",
    "- [markdown使用](#markdown使用)\n",
    "- [公式符号](#公式符号)\n",
    "- [常见词翻译](#常见词翻译)\n",
    "- [python知识点](#python知识点)\n",
    "    - [魔法方法](#魔法方法)\n",
    "        - [\\_\\_getitem\\_\\_](#\\_\\_getitem\\_\\_)\n",
    "        -[\\_\\_call\\_\\_](#\\_\\_call\\_\\_)\n",
    "- [numpy](#numpy)\n",
    "- [PyTorch](#PyTorch)\n",
    "    - [基础方法](#基础方法)\n",
    "    - [自动求导](#自动求导)\n",
    "        - [y.data.norm()](#y.data.norm())\n",
    "        - [with torch.no_grad()](#with&nbsp;torch.no_grad())\n",
    "    - [神经网络](#神经网络)  # nn库\n",
    "        - [nn.Conv2d卷积](#nn.Conv2d卷积)\n",
    "        - [nn.MaxPool2d](#nn.MaxPool2d)\n",
    "        - [nn.VagPool2d](#nn.MaxPool2d)\n",
    "        - [nn.Linear](#nn.Linear)\n",
    "        - [nn.ReLU&nbsp;vs&nbsp;F.ReLU](#nn.ReLU&nbsp;vs&nbsp;F.ReLU)\n",
    "    - [torchvision库](#torchvision库)\n",
    "    \n",
    "    \n",
    " [back to bottom](#Bottom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# markdown使用\n",
    "- [jupyter notebook中markdown使用](https://www.jianshu.com/p/576e3f1049f8)\n",
    "- [如何在Markdown文档中插入空格?](https://www.cnblogs.com/klchang/p/10203404.html)\n",
    "- [MarkDown用法详细介绍](https://blog.csdn.net/qq_27229113/article/details/73692171)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 公式符号\n",
    "更多用法：[https://blog.csdn.net/smilejiasmile/article/details/80670742](https://blog.csdn.net/smilejiasmile/article/details/80670742)\n",
    "\n",
    "J_\\alpha(x) = \\sum_{m=0}^\\infty \\frac{(-1)^m}{m! \\Gamma (m + \\alpha + 1)} {\\left({ \\frac{x}{2} }\\right)}^{2m + \\alpha} \\text {，行内公式示例}\n",
    "$$ J_\\alpha(x) = \\sum_{m=0}^\\infty \\frac{(-1)^m}{m! \\Gamma (m + \\alpha + 1)} {\\left({ \\frac{x}{2} }\\right)}^{2m + \\alpha} \\text {，行内公式示例} $$\n",
    "\\int_0^1 {x^2} \\,{\\rm d}x\n",
    "$$\\int_0^1 {x^2} \\,{\\rm d}x$$\n",
    "\\lim_{n \\to +\\infty} \\frac{1}{n(n+1)} \\quad and \\quad \\lim_{x\\leftarrow{示例}} \\frac{1}{n(n+1)}\n",
    "$$ \\lim_{n \\to +\\infty} \\frac{1}{n(n+1)} \\quad and \\quad \\lim_{x\\leftarrow{示例}} \\frac{1}{n(n+1)} $$\n",
    "\\sum_{i=1}^n \\frac{1}{i^2} \\quad and \\quad \\prod_{i=1}^n \\frac{1}{i^2} \\quad and \\quad \\bigcup_{i=1}^{2} R\n",
    "$$\\sum_{i=1}^n \\frac{1}{i^2} \\quad and \\quad \\prod_{i=1}^n \\frac{1}{i^2} \\quad and \\quad \\bigcup_{i=1}^{2} R$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 常见词翻译"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- criterion: 标准\n",
    "- optim: 使最优化\n",
    "- optimizer: 最优控制器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# python知识点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 魔法方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\_\\_getitem\\_\\_\n",
    "\n",
    "[python：__getitem__方法详解](https://blog.csdn.net/weixin_42557907/article/details/81589574)\n",
    "\n",
    "这个方法返回与指定键想关联的值。对序列来说，键应该是0~n-1的整数，其中n为序列的长度。对映射来说，键可以是任何类型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这个方法被调用\n",
      "This is python\n"
     ]
    }
   ],
   "source": [
    "class Tag:\n",
    "    def __init__(self):\n",
    "        self.change={'python':'This is python'}\n",
    " \n",
    "    def __getitem__(self, item):\n",
    "        print('这个方法被调用')\n",
    "        return self.change[item]\n",
    " \n",
    "a=Tag()\n",
    "print(a['python'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\_\\_call\\_\\_\n",
    "[Python __call__详解](https://www.jianshu.com/p/e1d95c4e1697?utm_source=oschina-app)\n",
    "\n",
    "``__call__``的作用相当于闭包中的内层方法，可以让一个类的实例对象当成方法调用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## np.transpose()\n",
    "\n",
    "多维数组的维度转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  1  2]\n",
      "  [ 3  4  5]]\n",
      "\n",
      " [[ 6  7  8]\n",
      "  [ 9 10 11]]]\n",
      "[[[ 0  1  2]\n",
      "  [ 6  7  8]]\n",
      "\n",
      " [[ 3  4  5]\n",
      "  [ 9 10 11]]]\n",
      "[[[ 0  6]\n",
      "  [ 3  9]]\n",
      "\n",
      " [[ 1  7]\n",
      "  [ 4 10]]\n",
      "\n",
      " [[ 2  8]\n",
      "  [ 5 11]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 0,1,2\n",
    "x = np.arange(12).reshape((2,2,3))\n",
    "print(x)\n",
    "# 1,0,2\n",
    "x = x.transpose(1,0,2)\n",
    "print(x)\n",
    "# 2,0,1\n",
    "x = np.transpose(x, (2,0,1))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前所有见过的类：\n",
    "- **torch.Tensor**\n",
    "    - A multi-dimensional array with support for autograd operations like backward(). Also holds the gradient w.r.t. the tensor.\n",
    "    - 支持autograd操作(如backward())的多维数组。也保持梯度w.r.t张量。\n",
    "- **nn.Module**\n",
    "    - Neural network module. Convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading,etc.\n",
    "    - 神经网络模块。方便的封装参数的方式，有帮助移动他们到GPU，导出，加载等。\n",
    "- **nn.Parameter** \n",
    "    - A kind of Tensor, that is automatically registered as a parameter when assigned as an attribute to a Module.\n",
    "    - 一种张量，当被指定为模块的属性时，自动作为参数注册。\n",
    "- **autograd.Function** \n",
    "    - Implements forward and backward definitions of an autograd operation. Every Tensor operation, creates at least a single Function node, that connects to functions thatcreated a Tensor and encodes its history.\n",
    "    - 实现autograd操作的向前和向后定义。每个张量操作，至少创建一个函数节点，连接到创建一个张量并编码其历史的函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**目前见到的一些方法的使用，仅展示，未整理：**  \n",
    "<font color=red>注：行与行之间没有关联，一般开头的变量都是一个张量，同名变量意义大部分情况相同</font>\n",
    "```python\n",
    "dtype = torch.float\n",
    "# 切换CPU、GPU\n",
    "device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda: 0\") \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x = torch.randn(64, 100, device=device, dtype=dtype)  # 随机数据\n",
    "w = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "h = x.mm(w1)\n",
    "h_relu = h.clamp(min=0)\n",
    "loss = (y_pred - y).pow(2).sum()\n",
    "loss.item()\n",
    "h_relu.t()\n",
    "grad_h_relu.clone()\n",
    "grad_h[h < 0] = 0\n",
    "loss.backward()\n",
    "with torch.no_grad():\n",
    "w1.grad.zero_()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y.data.norm() \n",
    "y的L2范数：$\\sum_{i=1}^ny^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.3757)\n",
      "tensor(4.3757, grad_fn=<SqrtBackward>)\n",
      "tensor([-947.1443, -188.2073,  567.7290], grad_fn=<MulBackward0>)\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x*2\n",
    "print(y.data.norm())\n",
    "print(torch.sqrt(torch.sum(torch.pow(y,2))))  #其实就是对y张量L2范数，先对y中每一项取平方，之后累加，最后取根号\n",
    "i=0\n",
    "while y.data.norm()<1000:\n",
    "  y = y*2\n",
    "  i+=1\n",
    "print(y)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with&nbsp;torch.no_grad()\n",
    "[https://www.jianshu.com/p/1cea017f5d11](https://www.jianshu.com/p/1cea017f5d11)  \n",
    "作用：使由requires_grad = True的tensor生成的新tensor的requires_grad=False，gead_fn=None,即不会对新tensor求导。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(10, 5, requires_grad = True)\n",
    "y = torch.randn(10, 5, requires_grad = True)\n",
    "z = torch.randn(10, 5, requires_grad = True)\n",
    "with torch.no_grad():\n",
    "    w = x + y + z\n",
    "    print(w.requires_grad)\n",
    "    print(w.grad_fn)\n",
    "print(w.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络\n",
    "\n",
    "神经网络可以通过 **torch.nn** 包来构建。\n",
    "\n",
    "现在对于自动梯度(**autograd**)有一些了解，神经网络是基于自动梯度 (autograd)来定义一些模型。一个 **nn.Module** 包括层和一个方法 **forward(input)** 它会返回输出(**output**)。\n",
    "\n",
    "一个典型的神经网络训练过程包括以下几点：\n",
    "1. 定义一个包含可训练参数的神经网络\n",
    "2. 迭代整个输入\n",
    "3. 通过神经网络处理输入\n",
    "4. 计算损失(loss)\n",
    "5. 反向传播梯度到神经网络的参数\n",
    "6. 更新网络的参数，典型的用一个简单的更新方法：weight = weight - learning_rate *gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Conv2d卷积"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一维卷积与二维卷积: [https://blog.csdn.net/qq_26369907/article/details/88366147](https://blog.csdn.net/qq_26369907/article/details/88366147)\n",
    "- **nn.Conv1d**: 一维的卷积能处理多维数据\n",
    "- **nn.Conv2d**: 二维卷积可以处理二维数据\n",
    "\n",
    "nn.Conv2d(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True))  \n",
    "nn.Conv1d(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True))  \n",
    "**参数**：\n",
    "- **in_channel**:　输入数据的通道数，例RGB图片通道数为3；\n",
    "- **out_channel**: 输出数据的通道数，这个根据模型调整；\n",
    "- **kennel_size**: 卷积核大小，可以是int，或tuple；kennel_size=2,意味着卷积大小(2,2)， kennel_size=（2,3），意味着卷积大小（2，3）即非正方形卷积\n",
    "- **stride**：步长，默认为1，与kennel_size类似，stride=2,意味着步长上下左右扫描皆为2， stride=（2,3），左右扫描步长为2，上下为3；\n",
    "- **padding**：零填充\n",
    "\n",
    "\n",
    "**卷积计算过程**：  \n",
    "h/w = (h/w - kennel_size + 2padding) / stride + 1  \n",
    "x = ([10,16,30,32]),其中h=30,w=32,对于卷积核长分别是 h:3，w:2 ；对于步长分别是h：2，w:1；padding默认0；  \n",
    "h = (30 - 3 + 20)/ 2 +1 = 27/2 +1 = 13+1 =14  \n",
    "w =(32 - 2 + 2*0)/ 1 +1 = 30/1 +1 = 30+1 =31  \n",
    "batch = 10, out_channel = 33  \n",
    "故： y= ([10, 33, 14, 31])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 例：nn.Conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 16, 30, 32, 34])\n",
      "Net_1D(\n",
      "  (layers): Sequential(\n",
      "    (0): Conv1d(16, 16, kernel_size=(3, 2, 2), stride=(2, 2, 1), padding=[2, 2, 2])\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n",
      "torch.Size([10, 16, 16, 18, 37])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x = torch.randn(10, 16, 30, 32, 34)\n",
    "# batch, channel , height , width\n",
    "print(x.shape)\n",
    "class Net_1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_1D, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=16, out_channels=16, kernel_size=(3, 2, 2), stride=(2, 2, 1), padding=[2,2,2]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        output = self.layers(x)\n",
    "        log_probs = F.log_softmax(output, dim=1)\n",
    "        return  log_probs\n",
    "\n",
    "n = Net_1D()  # in_channel,out_channel,kennel,\n",
    "print(n)\n",
    "y = n(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 例：nn.Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 16, 30, 32])\n",
      "Conv2d(16, 33, kernel_size=(3, 2), stride=(2, 1))\n",
      "torch.Size([10, 33, 14, 31])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.randn(10, 16, 30, 32) # batch, channel , height , width\n",
    "print(x.shape)\n",
    "m = nn.Conv2d(16, 33, (3, 2), (2,1))  # in_channel, out_channel ,kennel_size,stride\n",
    "print(m)\n",
    "y = m(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.MaxPool2d\n",
    "[https://www.jianshu.com/p/9d93a3391159](https://www.jianshu.com/p/9d93a3391159)\n",
    "\n",
    "\n",
    "class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)  \n",
    "**nn.Avg_pool**: 对于Avg_pool来说，参数和Max_pool是完全相同的，主要区别就是在kernel中取的是平均值操作。\n",
    "\n",
    "参数：\n",
    "- kernel_size(int or tuple): max pooling的窗口大小，\n",
    "- stride(int or tuple, optional): max pooling的窗口移动的步长。默认值是kernel_size\n",
    "- padding(int or tuple, optional): 输入的每一条边补充0的层数\n",
    "- dilation(int or tuple, optional): 一个控制窗口中元素步幅的参数\n",
    "- return_indices: 如果等于True，会返回输出最大值的序号，对于上采样操作会有帮助\n",
    "- ceil_mode: 如果等于True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"learn_img/16715697-3d69c563031e9ec4.webp\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "a = torch.randn(3,5,10)\n",
    "b = nn.MaxPool2d((5,1))  # kernel = 5 步长为1\n",
    "c = b(a)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pool操作并不改变张量的通道数\n",
    "x = torch.rand(1,3,7,7)\n",
    "out = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "out.forward(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1,3,7,7)\n",
    "out = nn.AvgPool2d(kernel_size=2,stride=2)\n",
    "out.forward(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Linear\n",
    "[https://blog.csdn.net/qq_42079689/article/details/102873766](https://blog.csdn.net/qq_42079689/article/details/102873766)  \n",
    "更详细：[https://blog.csdn.net/weixin_44613063/article/details/90274590](https://blog.csdn.net/weixin_44613063/article/details/90274590)\n",
    "\n",
    "PyTorch的 **nn.Linear()** 是用于设置网络中的全连接层的，需要注意的是全连接层的输入与输出都是二维张量，一般形状为\\[batch_size, size\\]\n",
    "$$y=xA^T+b$$\n",
    "\n",
    "计算步骤：\n",
    "```python\n",
    "@weak_script_method\n",
    "def forward(self, input):\n",
    "    return F.linear(input, self.weight, self.bias) \n",
    "```\n",
    "返回的是：**input * weight + bias**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12288])\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# in_features由输入张量的形状决定，out_features则决定了输出张量的形状 \n",
    "connected_layer = nn.Linear(in_features = 64*64*3, out_features = 1)\n",
    "\n",
    "# 假定输入的图像形状为[64,64,3]\n",
    "input = torch.randn(1,64,64,3)\n",
    "\n",
    "# 将四维张量转换为二维张量之后，才能作为全连接层的输入\n",
    "input = input.view(1,64*64*3)\n",
    "print(input.shape)\n",
    "output = connected_layer(input) # 调用全连接层\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.ReLU&nbsp;vs&nbsp;F.ReLU\n",
    "[https://blog.csdn.net/u011501388/article/details/86602275](https://blog.csdn.net/u011501388/article/details/86602275)\n",
    "\n",
    "在如下网络中，AlexNet_1与AlexNet_2实现的结果是一致的，但是可以看到将ReLU层添加到网络有两种不同的实现，即nn.ReLU和F.ReLU两种实现方法。\n",
    "其中nn.ReLU作为一个层结构，必须添加到nn.Module容器中才能使用，而F.ReLU则作为一个函数调用，看上去作为一个函数调用更方便更简洁。具体使用哪种方式，取决于编程风格。在PyTorch中,nn.X都有对应的函数版本F.X，但是并不是所有的F.X均可以用于forward或其它代码段中，因为当网络模型训练完毕时，在存储model时，在forward中的F.X函数中的参数是无法保存的。也就是说，在forward中，使用的F.X函数一般均没有状态参数，比如F.ReLU，F.avg_pool2d等，均没有参数，它们可以用在任何代码片段中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    " \n",
    "class AlexNet_1(nn.Module):\n",
    " \n",
    "    def __init__(self, num_classes=n):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "         )\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    " \n",
    "class AlexNet_2(nn.Module):\n",
    " \n",
    "    def __init__(self, num_classes=n):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "         )\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = F.ReLU(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torchvision库\n",
    "\n",
    "torchvision库简介（翻译）: [https://www.cnblogs.com/yjphhw/p/9773333.html](https://www.cnblogs.com/yjphhw/p/9773333.html)  \n",
    "torchvision是独立于pytorch的关于图像操作的一些方便工具库。  \n",
    "torchvision的详细介绍在：[https://pypi.org/project/torchvision/](https://pypi.org/project/torchvision/)  \n",
    "torchvision主要包括一下几个包：  \n",
    "- vision.datasets : 几个常用视觉数据集，可以下载和加载，这里主要的高级用法就是可以看源码如何自己写自己的Dataset的子类\n",
    "- vision.models : 流行的模型，例如 AlexNet, VGG, ResNet和Densenet以及 与训练好的参数。\n",
    "- vision.transforms : 常用的图像操作，例如：随机切割，旋转，数据类型转换，图像到tensor,numpy数组到tensor,tensor到图像等。\n",
    "- vision.utils : 用于把形似 (3 x H x W) 的张量保存到硬盘中，给一个mini-batch的图像可以产生一个图像格网。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bottom\n",
    "[[back to top](#Sections)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
