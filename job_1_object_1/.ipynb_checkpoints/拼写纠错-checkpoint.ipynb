{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思路\n",
    "+ 生成候选集合\n",
    "+ 过滤词（使用词典过滤）\n",
    "+ 使用Noisy Channer Model(语言模型)判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()  # 英文词库  # !!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成所有候选集合\n",
    "def generate_candidates(word):\n",
    "    \"\"\"\n",
    "    word: 给定的输入（错误的单词）\n",
    "    返回所有的（valid）候选集合\n",
    "    \"\"\"\n",
    "    # 生成编辑距离为1的单词（插入、删除、修改）\n",
    "    letters = [chr(i) for i in range(ord('a'), ord('z')+1)]\n",
    "    # 1. insert\n",
    "    inserts = [word[:i]+c+word[i:] for i in range(len(word)+1) for c in letters]\n",
    "    # 2. delete\n",
    "    deletes = [word[:i]+word[i+1:] for i in range(len(word))]\n",
    "    # 3. replace\n",
    "    replaces = [word[:i]+c+word[i+1:] for i in range(len(word)) for c in letters]\n",
    "    \n",
    "    candidates = set(inserts + deletes + replaces)\n",
    "    return [word for word in candidates if word in vocab]\n",
    "generate_candidates('apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "\n",
    "# 读取语料库，训练数据\n",
    "categories = reuters.categories()\n",
    "corpus = reuters.sents(categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建语言模型：Unigram, Bigram, Trigram\n",
    "# 此处使用：Bigram\n",
    "term_count = {}\n",
    "bigram_count = {}\n",
    "for doc in corpus:\n",
    "    doc = ['<s>'] + doc  # <s>开始符号，用于计算某个单词作为开头的概率\n",
    "    for i in range(0, len(doc) - 1):\n",
    "        term = doc[i]\n",
    "        bigram = ' '.join(doc[i:i+2])\n",
    "        \n",
    "        if term in term_count:\n",
    "            term_count[term] += 1\n",
    "        else:\n",
    "            term_count[term] = 1\n",
    "        \n",
    "        if bigram in bigram_count:\n",
    "            bigram_count[bigram] += 1\n",
    "        else:\n",
    "            bigram_count[bigram] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'raining': {'rainning': 0.5, 'raning': 0.5}}\n"
     ]
    }
   ],
   "source": [
    "# 用户拼写错误的概率统计 - channel probability\n",
    "channel_prob = {}\n",
    "# 读入一个拼写错误统计文件，包含正确的单词和可能拼错的词\n",
    "li_ru = {'raining': ['rainning', 'raning']}  # !!!!!!!!\n",
    "for correct, mistakes in li_ru.items():\n",
    "    channel_prob[correct] = {}\n",
    "    for mis in mistakes:\n",
    "        channel_prob[correct][mis] = 1.0/len(mistakes)\n",
    "print(channel_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "lines = [\"How are you !\", \"How old are you !\"]  # !!!!!!!!\n",
    "for line in lines:\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        if word not in vocab:\n",
    "            # 替换拼写错误的单词,获取编辑距离为1的单词\n",
    "            candidates = generate_candidates(word)\n",
    "            if len(candidates) == 0:  # !!!!!!!!\n",
    "            # 此处应加入但编辑距离为1的单词获取不到时获取编辑距离为2的单词\n",
    "            # 否则candidates为空，之后的程序执行不了\n",
    "                continue\n",
    "            probs = []\n",
    "            # 对于每一个candidate,计算它的score\n",
    "            # socre = P(correct)*P(mistake|correct)\n",
    "            #       = log(P(correct)) + log(P(mistake|correct))\n",
    "            # 返回score最大的condidate\n",
    "            for candi in candidates:\n",
    "                # 计算channel probability\n",
    "                if candi in channel_prob and word in channel_prob[candi]:\n",
    "                    prob += np.log(channel_prob[candi][word])\n",
    "                else:\n",
    "                    prob += np.log(0.0001)  # 此处正确的做法应该是加平滑项，使概率相加为1\n",
    "                # 语言模型的概率\n",
    "                idx = items[2].index(word) + 1\n",
    "                # 此处为语言模型公式，else为概率为零时的情况（平滑项使用的Add-one Smothing）\n",
    "                if items[2] [idx - 1] in bigram_count and candi in bigram_count[items[2][idx - 1]]:\n",
    "                    prob += np.log((bigram_count[items[2][idx - 1][candi] + 1.0]) / (\n",
    "                    term_count[bigram_count[items[2][idx - 1]]] + V))\n",
    "                else:\n",
    "                    prob += np.log(1.0 / V)\n",
    "                # 此处计算出的是语言模型的预测概率，即该词在该句中出现的概率，还应该乘上当前词错写成该词的概率（之前已计算出来）\n",
    "                # 目前的相当于认为候选词的条件概率[ p(text) ]相同\n",
    "                # p(text|source) ∝ p(source|text) * p(text)\n",
    "                probs.append(prob)\n",
    "            max_idx = probs.index(max(probs))\n",
    "            print(word, candidates[max_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
