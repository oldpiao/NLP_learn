{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分数（5）\n",
    "import json\n",
    "\n",
    "\n",
    "def read_corpus():\n",
    "    \"\"\"\n",
    "    读取给定的语料库，并把问题列表和答案列表分别写入到 qlist, alist 里面。 在此过程中，不用对字符换做任何的处理（这部分需要在 Part 2.3里处理）\n",
    "    qlist = [\"问题1\"， “问题2”， “问题3” ....]\n",
    "    alist = [\"答案1\", \"答案2\", \"答案3\" ....]\n",
    "    务必要让每一个问题和答案对应起来（下标位置一致）\n",
    "    \"\"\"\n",
    "    qjson = 'data/train-v3.0.json'\n",
    "    with open(qjson, 'r', encoding='utf-8') as rf:\n",
    "        qdict = json.loads(rf.read())\n",
    "    qlist, alist = [], []\n",
    "    for data in qdict['data']:\n",
    "        # print(data['title'])\n",
    "        for paragraph in data['paragraphs']:\n",
    "            for qa in paragraph['qas']:\n",
    "                # print(len(qa['answers']), qa)\n",
    "                if len(qa['answers'])==1:\n",
    "                    qlist.append(qa['question'])\n",
    "                    alist.append(qa['answers'][0]['text'])\n",
    "                # else:\n",
    "                #     alist.append('')\n",
    "            # print(paragraph['context'])\n",
    "    assert len(qlist) == len(alist)  # 确保长度一样\n",
    "    return qlist, alist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86821\n"
     ]
    }
   ],
   "source": [
    "qlist, alist = read_corpus()\n",
    "print(len(qlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "903411 63780\n",
      "['When', 'did', 'Beyonce', 'start', 'becoming', 'popular?', 'What', 'areas', 'did', 'Beyonce', 'compete', 'in', 'when', 'she', 'was', 'growing', 'up?', 'When', 'did', 'Beyonce', 'leave', \"Destiny's\", 'Child', 'and', 'become', 'a', 'solo', 'singer?', 'In', 'what', 'city', 'and', 'state', 'did', 'Beyonce', '', 'grow', 'up?', '', 'In', 'which', 'decade', 'did', 'Beyonce', 'become', 'famous?', 'In', 'what', 'R&B', 'group', 'was', 'she', 'the', 'lead', 'singer?', 'What', 'album', 'made', 'her', 'a', 'worldwide', 'known', 'artist?', 'Who', 'managed', 'the', \"Destiny's\", 'Child', 'group?', 'When', 'did', 'Beyoncé', 'rise', 'to', 'fame?', 'What', 'role', 'did', 'Beyoncé', 'have', 'in', \"Destiny's\", 'Child?', 'What', 'was', 'the', 'first', 'album', 'Beyoncé', 'released', 'as', 'a', 'solo', 'artist?', 'When', 'did', 'Beyoncé', 'release', 'Dangerously', 'in']\n"
     ]
    }
   ],
   "source": [
    "# 分数（10）\n",
    "# TODO: 统计一下在qlist 总共出现了多少个单词？ 总共出现了多少个不同的单词？\n",
    "#       这里需要做简单的分词，对于英文我们根据空格来分词即可，其他过滤暂不考虑（只需分词）\n",
    "q_total = [word for each_q in qlist for word in each_q.split(' ') ]\n",
    "q_total_set = set(q_total)\n",
    "print(len(q_total), len(q_total_set))\n",
    "print(q_total[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: 统计一下qlist中每个单词出现的频率，并把这些频率排一下序，然后画成plot. 比如总共出现了总共7个不同单词，而且每个单词出现的频率为 4, 5,10,2, 1, 1,1\n",
    "#       把频率排序之后就可以得到(从大到小) 10, 5, 4, 2, 1, 1, 1. 然后把这7个数plot即可（从大到小）\n",
    "#       需要使用matplotlib里的plot函数。y轴是词频\n",
    "%matplotlib inline\n",
    "\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "qs_counter = collections.Counter(q_total)\n",
    "qs, qs_c = zip(*qs_counter.most_common())\n",
    "plt.plot(range(100), qs_c[:100]);\n",
    "# plt.plot(range(len(qs_c)), qs_c);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO： 从上面的图中能观察到什么样的现象？ 这样的一个图的形状跟一个非常著名的函数形状很类似，能所出此定理吗？ \n",
    "#       hint: [XXX]'s law\n",
    "# 指数函数\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qlist出现次数最多的TOP 10单词及词频:\n",
      " [('the', 60959), ('What', 36994), ('of', 33300), ('', 29438), ('in', 21267), ('to', 17579), ('was', 17041), ('is', 16165), ('did', 15624), ('what', 11255)]\n",
      "alist出现次数最多的TOP 10单词及词频:\n",
      " [('the', 13211), ('of', 8892), ('and', 8172), ('to', 3440), ('a', 3294), ('in', 2838), ('The', 1769), ('or', 1261), ('for', 1032), ('million', 853)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: 在qlist和alist里出现次数最多的TOP 10单词分别是什么？ \n",
    "a_total = [word for each_a in alist for word in each_a.split(' ') ]\n",
    "a_total_set = set(a_total)\n",
    "as_counter = collections.Counter(a_total)\n",
    "print(\"qlist出现次数最多的TOP 10单词及词频:\\n\", qs_counter.most_common(10))\n",
    "print(\"alist出现次数最多的TOP 10单词及词频:\\n\", as_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 文本预处理\n",
    "次部分需要尝试做文本的处理。在这里我们面对的是英文文本，所以任何对英文适合的技术都可以考虑进来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "class DictTree(object):\n",
    "    def __init__(self, name='root', word=None, info=None):\n",
    "        self.name = name\n",
    "        self.info = info\n",
    "        self.word = word\n",
    "        self.children = {}\n",
    "    \n",
    "    def __str__(self):\n",
    "        return json.dumps({self.word: self.info}, ensure_ascii=False)\n",
    "    \n",
    "    def set_info(self, info):\n",
    "        self.info = info\n",
    "    \n",
    "    def find_child(self, name):\n",
    "        \"\"\"查询子节点是否存在\"\"\"\n",
    "        if name in self.children.keys():\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def add_word(self, chars, info, pointer=0):\n",
    "        \"\"\"将数据创建成字典树\"\"\"\n",
    "        if len(chars[pointer:]) <= 0:\n",
    "            self.info = info\n",
    "            self.word = chars\n",
    "        else:\n",
    "            if not self.find_child(chars[pointer]):\n",
    "                self.children[chars[pointer]] = DictTree(chars[pointer])\n",
    "            self.children[chars[pointer]].add_word(chars, info, pointer + 1)\n",
    "    \n",
    "    def add_words(self, words, infos):\n",
    "        \"\"\"添加多组词\"\"\"\n",
    "        for word, info in zip(words, infos):\n",
    "            self.add_word(word, info)\n",
    "            \n",
    "    def add_words2(self, words):\n",
    "        \"\"\"添加多组词，不添加此相关信息\"\"\"\n",
    "        for word in words:\n",
    "            self.add_word(word, None)\n",
    "    \n",
    "    def cut_word(self, chars):\n",
    "        \"\"\"\n",
    "        查询字典树中是否有某个词\n",
    "        return: 没有该词会返回空，有该值会返回该词的信息\n",
    "        \"\"\"\n",
    "        if len(chars) == 0:\n",
    "            if self.word is None:\n",
    "                return None\n",
    "            else:\n",
    "                return self\n",
    "        elif self.find_child(chars[0]):\n",
    "            return self.children[chars[0]].cut_word(chars[1:])\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def _cut_words(self, chars):\n",
    "        \"\"\"\n",
    "        查询字典树中是否有某个词\n",
    "        return: 没有该词会返回空，有该值会返回该词的信息\n",
    "        \"\"\"\n",
    "        words = set()\n",
    "        if len(chars) == 0:\n",
    "            return words\n",
    "        if self.word is not None:\n",
    "            words.add(self)\n",
    "        if self.find_child(chars[0]):\n",
    "            words.update(self.children[chars[0]]._cut_words(chars[1:]))\n",
    "        return words\n",
    "        \n",
    "    def cut_words(self, sentence):\n",
    "        \"\"\"查询字典树\"\"\"\n",
    "        words = set()\n",
    "        if len(sentence) == 0:\n",
    "            return words\n",
    "        for n in range(len(sentence)):\n",
    "            words.update(self._cut_words(sentence[n:]))\n",
    "        return words\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"输出成dict\"\"\"\n",
    "        children = {}\n",
    "        for child_name, child_node in self.children.items():\n",
    "            children[child_name] = child_node.to_dict()\n",
    "        return {\n",
    "            'name': self.name,\n",
    "            'info': self.info,\n",
    "            'children': children,\n",
    "        }\n",
    "    \n",
    "    def read_dict(self, node_dict):\n",
    "        \"\"\"从dict中读入\"\"\"\n",
    "        self.name = node_dict['name']\n",
    "        self.info = node_dict['info']\n",
    "        for child_name, child_dict in node_dict['children'].items():\n",
    "            self.children[child_name] = Node(None).read_dict(child_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/英文停用词.txt', 'r', encoding='utf-8') as f:\n",
    "    words = set(f.read().split('\\n') + [''])\n",
    "stop_words = DictTree()\n",
    "stop_words.add_words2(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "def cut(sentence):\n",
    "    \"\"\"英文分词\"\"\"\n",
    "    words = []\n",
    "    for word in re.split(r'\\s+', sentence):\n",
    "        find_punc = re.search('^(.+)([,.?!:;]+)$', word)\n",
    "        if find_punc is not None:\n",
    "            words.append(find_punc.groups()[0])\n",
    "            words.append(find_punc.groups()[1])\n",
    "        else:\n",
    "            words.append(word)\n",
    "    return words\n",
    "\n",
    "def lower_case(words):\n",
    "    \"\"\"全部转为小写字母\"\"\"\n",
    "    return [i.lower() for i in words]\n",
    "\n",
    "def del_stop_words(words):\n",
    "    \"\"\"去除停用词\"\"\"\n",
    "    # 不在停用词表的词\n",
    "    return [i for i in words if stop_words.cut_word(i) is None]\n",
    "\n",
    "def porter_stemming(words):\n",
    "    return [porter_stemmer.stem(w) for w in words]\n",
    "\n",
    "def number2word(words):\n",
    "    return ['#number' if re.search(r'\\d+\\.*\\d*', word) else word for word in words]\n",
    "\n",
    "def get_counter(all_words):\n",
    "    \"\"\"获取词频\"\"\"\n",
    "    return collections.Counter([word for words in all_words for word in words])\n",
    "\n",
    "def del_low_freq_words(all_words, w_counter, low_freq=1):\n",
    "    \"\"\"删除低频词\n",
    "    param: all_words: [[words], ...]\n",
    "    param: low_freq: 删除低于该频率的词\n",
    "    \"\"\"\n",
    "    low_freq_words = [i[0] for i in w_counter.most_common() if i[1] <= low_freq]\n",
    "    lfw_tree = DictTree()\n",
    "    lfw_tree.add_words2(low_freq_words)\n",
    "    print(len(low_freq_words), len(w_counter.most_common()))\n",
    "    return [[word for word in words if lfw_tree.cut_word(word) is None] for words in all_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'bjm', '.', '']\n",
      "['I', 'bjm', '.']\n",
      "2 6\n",
      "[['1', '1', '3', '3'], ['1', '3', '4', '4', '5', '5']]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(cut('I am bjm. '))\n",
    "print(del_stop_words(cut('I am bjm. ')))\n",
    "test_all_words = [['1','1','2','3','3'],['1','3','4','4','5','5', '8']]\n",
    "print(del_low_freq_words(test_all_words, get_counter(test_all_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34927 40382\n",
      "37235 40093\n"
     ]
    }
   ],
   "source": [
    "def sentence2words(sentence):\n",
    "    \"\"\"单个句子的预处理，处理为词\"\"\"\n",
    "    words = cut(sentence)\n",
    "    dsw_words = del_stop_words(words)\n",
    "    # ps_words = porter_stemming(dsw_words)\n",
    "    lc_words = lower_case(dsw_words)\n",
    "    dsw_words = del_stop_words(lc_words)\n",
    "    n2w_words = number2word(dsw_words)\n",
    "    return n2w_words\n",
    "\n",
    "def preprocess(sentences):\n",
    "    \"\"\"数据处理\"\"\"\n",
    "    all_words = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence2words(sentence)\n",
    "        all_words.append(words)\n",
    "    w_counter = get_counter(all_words)\n",
    "    all_words = del_low_freq_words(all_words, w_counter, low_freq=10)\n",
    "    return all_words\n",
    "all_q_words = preprocess(qlist)\n",
    "all_a_words = preprocess(alist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When did Beyonce start becoming popular?\n",
      "['beyonce', 'start', 'popular', '?']\n",
      "What areas did Beyonce compete in when she was growing up?\n",
      "['beyonce', 'compete', 'growing', '?']\n",
      "When did Beyonce leave Destiny's Child and become a solo singer?\n",
      "['beyonce', 'leave', \"destiny's\", 'child', 'solo', 'singer', '?']\n",
      "In what city and state did Beyonce  grow up? \n",
      "['city', 'beyonce', 'grow', '?']\n",
      "In which decade did Beyonce become famous?\n",
      "['decade', 'beyonce', 'famous', '?']\n",
      "In what R&B group was she the lead singer?\n",
      "['r&b', 'lead', 'singer', '?']\n",
      "What album made her a worldwide known artist?\n",
      "['album', 'worldwide', 'artist', '?']\n",
      "Who managed the Destiny's Child group?\n",
      "['managed', \"destiny's\", 'child', '?']\n",
      "When did Beyoncé rise to fame?\n",
      "['beyoncé', 'rise', 'fame', '?']\n",
      "What role did Beyoncé have in Destiny's Child?\n",
      "['role', 'beyoncé', \"destiny's\", 'child', '?']\n"
     ]
    }
   ],
   "source": [
    "# 打印前十个问题，及其分词\n",
    "for i, j in zip(qlist, all_q_words[:10]):\n",
    "    print('%s\\n%s' % (i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 若返回空则证明数字已被处理为#number\n",
    "i = 0\n",
    "for q_words in all_q_words:\n",
    "    for word in q_words:\n",
    "        if re.search(r'^\\d+$', word) is not None:\n",
    "            i += 1\n",
    "            if i >= 10:\n",
    "                break\n",
    "            print(word, q_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 在前面步骤里，我们删除了出现次数比较少的单词，那你选择的阈值是多少（小于多少的去掉？）， 这个阈值是根据什么来选择的？ \n",
    "#  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 文本表示\n",
    "当我们做完关键的预处理过程之后，就需要把每一个文本转换成向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分数（10）\n",
    "\n",
    "# TODO: 把qlist中的每一个问题字符串转换成tf-idf向量, \n",
    "# 转换之后的结果存储在X矩阵里。 X的大小是： N* D的矩阵。 \n",
    "# 这里N是问题的个数（样本个数），\n",
    "#       D是字典库的大小。 \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "q_corpus = [' '.join(q_words) for q_words in all_q_words]\n",
    "\n",
    "# 直接生成tf-idf值\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit_transform(q_corpus).todense()\n",
    "\n",
    "# tfidf_matrix = tfidf_vec.fit_transform(q_corpus).todense()\n",
    "# vectorizer =  # 定义一个tf-idf的vectorizer\n",
    "\n",
    "X = vectorizer.transform(q_corpus)  # 结果存放在X矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(86821, 5293)\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "col: [4572, 3642, 498]\n",
      "row: [0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# scipy.sparse.csr.csr_matrix\n",
    "# 结果X采用了csr_matrix矩阵压缩技术\n",
    "print(type(X))\n",
    "print(X.shape)\n",
    "# print([i for i in X[0].toarray() if i != 0])\n",
    "print(X[0].toarray())\n",
    "team_coo = X[0].tocoo()\n",
    "item =list(team_coo.col.reshape(-1))\n",
    "user =list(team_coo.row.reshape(-1))\n",
    "print(\"col:\",item)\n",
    "print(\"row:\",user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# X_2 = [[[n, j] for n, j in enumerate(i) if j != 0] for i in tfidf_matrix.toarray()]\n",
    "# print(X_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 矩阵X有什么特点？ 计算一下它的稀疏度\n",
    "# sparsity = len(X[X[i]!=0, i])/len(X[i])\n",
    "# print (sparsity)  # 打印出稀疏度(sparsity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 对于用户的输入问题，找到相似度最高的TOP5问题，并把5个潜在的答案做返回\n",
    "\n",
    "在该步骤中已做了倒排表优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inverted_list(tfidf_x):\n",
    "    \"\"\"生成倒查表\"\"\"\n",
    "    inverted_list = [set() for i in range(tfidf_x.shape[1])]\n",
    "    for n, i in enumerate(tfidf_x, 0):\n",
    "        for j in i.tocoo().col.reshape(-1):\n",
    "            inverted_list[j].add(n)\n",
    "    return inverted_list\n",
    "\n",
    "inverted_list = get_inverted_list(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{29120, 29122, 29154, 29092, 29093, 28997, 29286, 29288, 29289, 29319, 69817, 29015, 29145, 47131, 47133, 47134, 47135}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class InvertedList(object):\n",
    "    \n",
    "    def __init__(self, inverted_list):\n",
    "        self.inverted_list = inverted_list\n",
    "    \n",
    "    def select(self, data_list):\n",
    "        result = []\n",
    "        for i in data_list:\n",
    "            result.extend(self.inverted_list[i])\n",
    "        return result\n",
    "    \n",
    "    def select_csr_matrix(self, csr_m):\n",
    "        return self.select(csr_m.tocoo().col.reshape(-1))\n",
    "    \n",
    "def get_top_use_counter(words, n=10):\n",
    "    \"\"\" 匹配出现词频最高的至少n个词，统一词频的一个被获取全都会被获取\n",
    "    param: words: 一系列单词，会统计其词频\n",
    "    param: n: 在去重后总词数大于n时,最少匹配n个词\n",
    "    \"\"\"\n",
    "    data_counter = collections.Counter(words)\n",
    "    # print(data_counter.most_common(10))\n",
    "    result, next_c = [], data_counter.most_common(1)[0][1]\n",
    "    for i in data_counter.most_common():\n",
    "        if n < 0 and i[1] != next_c:\n",
    "            break\n",
    "        result.append(i)\n",
    "        n -= 1\n",
    "        next_c = i[1]\n",
    "    return result\n",
    "\n",
    "def similarity(a, b):\n",
    "    return a.toarray().dot(b.toarray().T)[0][0]\n",
    "    \n",
    "il = InvertedList(inverted_list)\n",
    "print(il.inverted_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2854606516948644"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity(X[0], X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "\n",
    "# 分数（10）\n",
    "def top5results(input_q):\n",
    "    \"\"\"\n",
    "    给定用户输入的问题 input_q, 返回最有可能的TOP 5问题。这里面需要做到以下几点：\n",
    "    1. 对于用户的输入 input_q 首先做一系列的预处理，然后再转换成tf-idf向量（利用上面的vectorizer)\n",
    "    2. 计算跟每个库里的问题之间的相似度\n",
    "    3. 找出相似度最高的top5问题的答案\n",
    "    \"\"\"\n",
    "#     tfidf_matrix.todense()\n",
    "    tfidf_q = vectorizer.transform([input_q])\n",
    "    q_list = il.select_csr_matrix(tfidf_q)\n",
    "    q_top = get_top_use_counter(q_list)\n",
    "    # 之后跟语句相似度计算\n",
    "    similarities = [(i[0], similarity(tfidf_q, X[i[0]])) for i in q_top]\n",
    "    # 排序\n",
    "    df_sim = pd.DataFrame(similarities, columns=['q_num', 'similarity'])\n",
    "    df_sim = df_sim.sort_values(by=['similarity'], ascending=False)\n",
    "    # print(similarities)\n",
    "    # print(q_top)\n",
    "    # print(tfidf_q)\n",
    "    # top_idxs存放相似度最高的（存在qlist里的）问题的下表 \n",
    "    # hint: 利用priority queue来找出top results. 思考为什么可以这么做？ \n",
    "    top_idxs = list(df_sim.head(5)['q_num'].values)\n",
    "    # for i in top_idxs:\n",
    "    #     print(\"%d,question: %s\" %(i, qlist[i]))\n",
    "    #     print(\"%d,answer: %s\" % (i, alist[i]))\n",
    "    return alist[top_idxs[0]]  # 返回相似度最高的问题对应的答案，作为TOP5答案    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lead singer\n",
      "10,question: What was the first album Beyoncé released as a solo artist?\n",
      "10,true answer: Dangerously in Love\n",
      "10,pred answer:Dangerously in Love\n",
      "20,question: Which album was darker in tone from her previous work?\n",
      "20,true answer: Beyoncé\n",
      "20,pred answer:Beyoncé\n"
     ]
    }
   ],
   "source": [
    "print(top5results(\"What role did Beyoncé have in Destiny's Child?\"))\n",
    "for i in [10, 20]:\n",
    "    print(\"%d,question: %s\" %(i, qlist[i]))\n",
    "    print(\"%d,true answer: %s\" % (i, alist[i]))\n",
    "    print(\"%d,pred answer:%s\" % (i, top5results(qlist[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 利用倒排表的优化。 \n",
    "上面的算法，一个最大的缺点是每一个用户问题都需要跟库里的所有的问题都计算相似度。假设我们库里的问题非常多，这将是效率非常低的方法。 这里面一个方案是通过倒排表的方式，先从库里面找到跟当前的输入类似的问题描述。然后针对于这些candidates问题再做余弦相似度的计算。这样会节省大量的时间。\n",
    "\n",
    "#### 答：在上一步中已经做了倒排表优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 分数（5）\n",
    "\n",
    "# TODO: 上面的top5results算法的时间复杂度和空间复杂度分别是多少？\n",
    "\n",
    "时间复杂度 = O()， 空间复杂度 = O()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
