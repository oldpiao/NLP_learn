{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 分数（5）\n",
    "import json\n",
    "\n",
    "\n",
    "def read_corpus():\n",
    "    \"\"\"\n",
    "    读取给定的语料库，并把问题列表和答案列表分别写入到 qlist, alist 里面。 在此过程中，不用对字符换做任何的处理（这部分需要在 Part 2.3里处理）\n",
    "    qlist = [\"问题1\"， “问题2”， “问题3” ....]\n",
    "    alist = [\"答案1\", \"答案2\", \"答案3\" ....]\n",
    "    务必要让每一个问题和答案对应起来（下标位置一致）\n",
    "    \"\"\"\n",
    "    qjson = 'data/train-v3.0.json'\n",
    "    with open(qjson, 'r', encoding='utf-8') as rf:\n",
    "        qdict = json.loads(rf.read())\n",
    "    qlist, alist = [], []\n",
    "    for data in qdict['data']:\n",
    "        # print(data['title'])\n",
    "        for paragraph in data['paragraphs']:\n",
    "            for qa in paragraph['qas']:\n",
    "                # print(len(qa['answers']), qa)\n",
    "                if len(qa['answers'])==1:\n",
    "                    qlist.append(qa['question'])\n",
    "                    alist.append(qa['answers'][0]['text'])\n",
    "                # else:\n",
    "                #     alist.append('')\n",
    "            # print(paragraph['context'])\n",
    "    assert len(qlist) == len(alist)  # 确保长度一样\n",
    "    return qlist, alist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86821\n"
     ]
    }
   ],
   "source": [
    "qlist, alist = read_corpus()\n",
    "print(len(qlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "903411 63780\n",
      "['When', 'did', 'Beyonce', 'start', 'becoming', 'popular?', 'What', 'areas', 'did', 'Beyonce', 'compete', 'in', 'when', 'she', 'was', 'growing', 'up?', 'When', 'did', 'Beyonce', 'leave', \"Destiny's\", 'Child', 'and', 'become', 'a', 'solo', 'singer?', 'In', 'what', 'city', 'and', 'state', 'did', 'Beyonce', '', 'grow', 'up?', '', 'In', 'which', 'decade', 'did', 'Beyonce', 'become', 'famous?', 'In', 'what', 'R&B', 'group', 'was', 'she', 'the', 'lead', 'singer?', 'What', 'album', 'made', 'her', 'a', 'worldwide', 'known', 'artist?', 'Who', 'managed', 'the', \"Destiny's\", 'Child', 'group?', 'When', 'did', 'Beyoncé', 'rise', 'to', 'fame?', 'What', 'role', 'did', 'Beyoncé', 'have', 'in', \"Destiny's\", 'Child?', 'What', 'was', 'the', 'first', 'album', 'Beyoncé', 'released', 'as', 'a', 'solo', 'artist?', 'When', 'did', 'Beyoncé', 'release', 'Dangerously', 'in']\n"
     ]
    }
   ],
   "source": [
    "# 分数（10）\n",
    "# TODO: 统计一下在qlist 总共出现了多少个单词？ 总共出现了多少个不同的单词？\n",
    "#       这里需要做简单的分词，对于英文我们根据空格来分词即可，其他过滤暂不考虑（只需分词）\n",
    "q_total = [word for each_q in qlist for word in each_q.split(' ') ]\n",
    "q_total_set = set(q_total)\n",
    "print(len(q_total), len(q_total_set))\n",
    "print(q_total[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xt03OV95/H3dy6SRrYlWbIwsmwjg52LIQViYewkTdo4\nWZy2CZwuod42xSd1oVvoNu32nBa22z96znKanDalyW6hZSHBkDTES9LgprksMUnTs8EGcQlgG2Nh\nA7Z8kfBNvukyM9/94/eMGOsu2eOR9Pu8zpnMb57fZZ4HyHz0PM/vYu6OiIhIsUS5KyAiIlOPwkFE\nRIZQOIiIyBAKBxERGULhICIiQygcRERkCIWDiIgMoXAQEZEhFA4iIjJEqtwVmKx58+Z5S0tLuash\nIjKtPPfcc2+7e+NY203bcGhpaaGtra3c1RARmVbM7M3xbKdhJRERGULhICIiQygcRERkCIWDiIgM\noXAQEZEhFA4iIjKEwkFERIaIXTg8+8ZR/uaHu8jl9XhUEZGRjCsczKzOzB43s1fNbKeZrTazejN7\n0sx2h/e5RdvfbWbtZrbLzG4oKl9hZi+HdV82MwvllWb2zVC+zcxaLnRDC1586zj/68ftnO3Pleor\nRESmvfH2HL4E/MDd3wNcDewE7gK2uPsyYEv4jJktB9YBVwJrgfvMLBmOcz9wG7AsvNaG8g3AMXdf\nCtwLfOE82zWiqoqoKmf7FA4iIiMZMxzMrBb4MPAQgLv3uftx4EZgY9hsI3BTWL4ReMzde919L9AO\nrDSzJqDG3be6uwOPDNqncKzHgTWFXsWFlklH4dCjnoOIyIjG03NYAnQBXzWzF8zsQTObBcx394Nh\nm0PA/LDcDOwr2n9/KGsOy4PLz9nH3bPACaBhcEXM7HYzazOztq6urvG0b4hCOGhYSURkZOMJhxTw\nfuB+d78WOE0YQioIPYGSz/C6+wPu3ururY2NY95UcFiZiqjJGlYSERnZeMJhP7Df3beFz48ThcXh\nMFREeO8M6zuARUX7LwxlHWF5cPk5+5hZCqgFjky0MeNRpZ6DiMiYxgwHdz8E7DOzd4eiNcAOYDOw\nPpStB54Iy5uBdeEMpCVEE8/PhCGobjNbFeYTbh20T+FYNwNPhd7IBadhJRGRsY33eQ7/Bfi6mVUA\ne4DPEgXLJjPbALwJ3ALg7tvNbBNRgGSBO9298Et8B/AwkAG+H14QTXY/ambtwFGis51KIhPOVurR\nsJKIyIjGFQ7u/iLQOsyqNSNsfw9wzzDlbcBVw5T3AJ8eT13Ol3oOIiJji90V0goHEZGxxS4cdBGc\niMjYYhcOughORGRssQuHdDJBKmEaVhIRGUXswgGi3sPZvny5qyEiMmXFMxwqkuo5iIiMIrbhoDkH\nEZGRxTMc0kmdrSQiMopYhkNVWsNKIiKjiWU4ZBQOIiKjimc4aM5BRGRU8QwHzTmIiIwqluGgOQcR\nkdHFMhwyFQkNK4mIjCKe4ZBOckbDSiIiI4ptOJztz1Gih82JiEx7sQyHqook7tCb1f2VRESGE8tw\n0G27RURGF+tw0BlLIiLDi2c46GlwIiKjimU4VKnnICIyqliGg+YcRERGF89wGBhW0tlKIiLDGVc4\nmNkbZvaymb1oZm2hrN7MnjSz3eF9btH2d5tZu5ntMrMbispXhOO0m9mXzcxCeaWZfTOUbzOzlgvb\nzHNpQlpEZHQT6Tn8srtf4+6t4fNdwBZ3XwZsCZ8xs+XAOuBKYC1wn5klwz73A7cBy8JrbSjfABxz\n96XAvcAXJt+ksWnOQURkdOczrHQjsDEsbwRuKip/zN173X0v0A6sNLMmoMbdt3p0afIjg/YpHOtx\nYE2hV1EKhWGlHp2tJCIyrPGGgwM/MrPnzOz2UDbf3Q+G5UPA/LDcDOwr2nd/KGsOy4PLz9nH3bPA\nCaBhAu2YEA0riYiMLjXO7T7k7h1mdgnwpJm9WrzS3d3MSn6johBMtwMsXrx40sdROIiIjG5cPQd3\n7wjvncA/AyuBw2GoiPDeGTbvABYV7b4wlHWE5cHl5+xjZimgFjgyTD0ecPdWd29tbGwcT9WHVZmK\nmq2L4EREhjdmOJjZLDObU1gG/gPwCrAZWB82Ww88EZY3A+vCGUhLiCaenwlDUN1mtirMJ9w6aJ/C\nsW4GnvIS3jI1kTCq0nqmg4jISMYzrDQf+OcwP5wC/sndf2BmzwKbzGwD8CZwC4C7bzezTcAOIAvc\n6e6FX+E7gIeBDPD98AJ4CHjUzNqBo0RnO5VURk+DExEZ0Zjh4O57gKuHKT8CrBlhn3uAe4YpbwOu\nGqa8B/j0OOp7weg50iIiI4vlFdIQPdNBPQcRkeHFNhwy6aTmHERERhDrcFDPQURkePENhwrNOYiI\njCS24VCVTnK2X3dlFREZTmzDQXMOIiIji3U4aFhJRGR48Q0HncoqIjKi2IZDlc5WEhEZUWzDIZNO\n0pfNk8uX/GayIiLTTnzDoSJquialRUSGim846JkOIiIjim04DDxHWmcsiYgMEdtwGHiOtHoOIiJD\nxDccNKwkIjIihYOGlUREhohtOFRVqOcgIjKS2IZDoeegOQcRkaFiHw7qOYiIDBXfcAjDSmc05yAi\nMkRsw0HXOYiIjCy24aA5BxGRkcU2HNJJI5kwzTmIiAwjtuFgZuGBP3pUqIjIYOMOBzNLmtkLZvbd\n8LnezJ40s93hfW7RtnebWbuZ7TKzG4rKV5jZy2Hdl83MQnmlmX0zlG8zs5YL18SR6ZkOIiLDm0jP\n4XPAzqLPdwFb3H0ZsCV8xsyWA+uAK4G1wH1mlgz73A/cBiwLr7WhfANwzN2XAvcCX5hUayYoU5HQ\nnIOIyDDGFQ5mthD4VeDBouIbgY1heSNwU1H5Y+7e6+57gXZgpZk1ATXuvtXdHXhk0D6FYz0OrCn0\nKkpJz5EWERneeHsOfwf8KVA8QD/f3Q+G5UPA/LDcDOwr2m5/KGsOy4PLz9nH3bPACaBhnHWbtIyG\nlUREhjVmOJjZrwGd7v7cSNuEnkDJn7dpZrebWZuZtXV1dZ338TTnICIyvPH0HD4IfMrM3gAeAz5q\nZl8DDoehIsJ7Z9i+A1hUtP/CUNYRlgeXn7OPmaWAWuDI4Iq4+wPu3ururY2NjeNq4GgyFUnNOYiI\nDGPMcHD3u919obu3EE00P+XunwE2A+vDZuuBJ8LyZmBdOANpCdHE8zNhCKrbzFaF+YRbB+1TONbN\n4TtK3hPRnIOIyPBS57Hv54FNZrYBeBO4BcDdt5vZJmAHkAXudPfCL/AdwMNABvh+eAE8BDxqZu3A\nUaIQKjnNOYiIDG9C4eDuPwF+EpaPAGtG2O4e4J5hytuAq4Yp7wE+PZG6XAgaVhIRGV5sr5AGDSuJ\niIwk3uFQEQ0rXYTpDRGRaSXW4VCVTpJ36Mvp/koiIsViHQ4Dt+3WzfdERM4R73AIT4M72dtf5pqI\niEwtsQ6Hd82fA8BL+0+UuSYiIlNLrMPhFxbWUl2R5OnXh1yMLSISa7EOh3QywXUt9Ty9R+EgIlIs\n1uEAsPqKBto7T9F5sqfcVRERmTIUDpdHdwbfuudomWsiIjJ1xD4crlxQw5zKlOYdRESKxD4cUskE\nK5fUs1XzDiIiA2IfDhDNO+x9+zSHTmjeQUQEFA5AFA4AT+95u8w1ERGZGhQOwHsvraGuOq15BxGR\nQOEAJBLG9Ut0vYOISIHCIVh9eQP7jp5l/7Ez5a6KiEjZKRyCVWHeYZuudxARUTgUvOuSOdRm0jyz\nV+EgIqJwCBIJ47qWep55Q+EgIqJwKHL9knr2vn2azm5d7yAi8aZwKHL95fUAbNPQkojEnMKhyPKm\nGmZVJDXvICKxp3AokkomWNFSr3AQkdgbMxzMrMrMnjGzn5vZdjP7y1Beb2ZPmtnu8D63aJ+7zazd\nzHaZ2Q1F5SvM7OWw7stmZqG80sy+Gcq3mVnLhW/q+Fy/pJ5dh09y9HRfuaogIlJ24+k59AIfdfer\ngWuAtWa2CrgL2OLuy4At4TNmthxYB1wJrAXuM7NkONb9wG3AsvBaG8o3AMfcfSlwL/CFC9C2Sbl+\nSTTv8KzOWhKRGBszHDxyKnxMh5cDNwIbQ/lG4KawfCPwmLv3uvteoB1YaWZNQI27b3V3Bx4ZtE/h\nWI8Dawq9iovtfQtrqUwlNLQkIrE2rjkHM0ua2YtAJ/Cku28D5rv7wbDJIWB+WG4G9hXtvj+UNYfl\nweXn7OPuWeAE0DDh1lwAlakk1y6uUziISKyNKxzcPefu1wALiXoBVw1a70S9iZIys9vNrM3M2rq6\nukr2PSuXNLD9wAm6e/pL9h0iIlPZhM5WcvfjwI+J5goOh6Eiwntn2KwDWFS028JQ1hGWB5efs4+Z\npYBaYMgtUt39AXdvdffWxsbGiVR9Qq5rmUve4eX9J0r2HSIiU9l4zlZqNLO6sJwBPg68CmwG1ofN\n1gNPhOXNwLpwBtISoonnZ8IQVLeZrQrzCbcO2qdwrJuBp0JvpCxaGmYB0HHsbLmqICJSVqlxbNME\nbAxnHCWATe7+XTN7GthkZhuAN4FbANx9u5ltAnYAWeBOd8+FY90BPAxkgO+HF8BDwKNm1g4cJTrb\nqWzm11RhBh3HFQ4iEk9jhoO7vwRcO0z5EWDNCPvcA9wzTHkbcNUw5T3Ap8dR34uiIpWgcXYlB08o\nHEQknnSF9Aia6jIcPKEb8IlIPCkcRtBcV6VhJRGJLYXDCJpqMxw83kMZ58VFRMpG4TCCBXUZzvbn\nOH5G1zqISPwoHEawoLYKgAOalBaRGFI4jKCpLgPAweOalBaR+FE4jGBBnXoOIhJfCocRzJtVSTpp\nHFDPQURiSOEwgkTCuLS2ShfCiUgsKRxGsaA2wwFd6yAiMaRwGMWCuoyGlUQklhQOo1hQV8Wh7h5y\neV0IJyLxonAYRVNthlze6TrZW+6qiIhcVAqHUeh0VhGJK4XDKBaEC+E0KS0icaNwGEVTra6SFpF4\nUjiMoqYqxayKpIaVRCR2FA6jMLNwOqvCQUTiReEwBj0RTkTiSOEwhgW1Veo5iEjsKBzGsKAuw9un\n+ujN5spdFRGRi0bhMIam8NCfQxpaEpEYUTiMoTlc69ChoSURiRGFwxgWN1QDsPvwqTLXRETk4hkz\nHMxskZn92Mx2mNl2M/tcKK83syfNbHd4n1u0z91m1m5mu8zshqLyFWb2clj3ZTOzUF5pZt8M5dvM\nrOXCN3VymusyNNdl2LrnSLmrIiJy0Yyn55AF/sTdlwOrgDvNbDlwF7DF3ZcBW8Jnwrp1wJXAWuA+\nM0uGY90P3AYsC6+1oXwDcMzdlwL3Al+4AG27IMyMVZc3sHXPEfK6O6uIxMSY4eDuB939+bB8EtgJ\nNAM3AhvDZhuBm8LyjcBj7t7r7nuBdmClmTUBNe6+1d0deGTQPoVjPQ6sKfQqpoIPXNHAsTP97Dp8\nstxVERG5KCY05xCGe64FtgHz3f1gWHUImB+Wm4F9RbvtD2XNYXlw+Tn7uHsWOAE0DPP9t5tZm5m1\ndXV1TaTq52X1FVFVnn5dQ0siEg/jDgczmw18C/gjd+8uXhd6AiUfc3H3B9y91d1bGxsbS/11AxbU\nZbisoZqfKRxEJCbGFQ5mliYKhq+7+7dD8eEwVER47wzlHcCiot0XhrKOsDy4/Jx9zCwF1AJT6pd4\n9eUNbNt7RE+FE5FYGM/ZSgY8BOx0978tWrUZWB+W1wNPFJWvC2cgLSGaeH4mDEF1m9mqcMxbB+1T\nONbNwFOhNzJlrL6igZM9WXYc6B57YxGRaS41jm0+CPw28LKZvRjK/hvweWCTmW0A3gRuAXD37Wa2\nCdhBdKbTne5euPfEHcDDQAb4fnhBFD6Pmlk7cJTobKcpZfXlYd5hz9u8b2FtmWsjIlJaNsX+QB+3\n1tZWb2tru6jf+dEv/oTL6qv56mdXXtTvFRG5UMzsOXdvHWs7XSE9Aasvb+DZN46RzeXLXRURkZJS\nOEzAB66Yx6neLC/uO17uqoiIlJTCYQJWX9HArIokn334WR789z30ZdWDEJGZSeEwAfWzKnjiDz7E\nisvm8j/+dSc3/N1Pee7NY+WulojIBadwmKCll8zm4c+u5KufvY5c3vmtB7fyb69dvKu1RUQuBoXD\nJP3yuy/h23d8gMvnzeZ3Nz7L914+OPZOIiLThMLhPMybXck3bl/FNYvq+IN/ep7/uWU3Pf16nKiI\nTH8Kh/NUm0nzyO9czyfe18QXn3yNj9/7b/zglYNM1+tHRERA4XBBZCqS/P1vvp+vbbieTDrJf/7a\n83xpy+5yV0tEZNIUDhfQh5bN43t/+Itc1zKXH24/XO7qiIhMmsLhAkslE6y6vIFdh7o505ctd3VE\nRCZF4VAC1yyqI+/w8v4T5a6KiMikKBxK4JpFdQC6zYaITFsKhxJomF3JovoMP9+vcBCR6UnhUCLX\nLJrLi28pHERkelI4lMg1i+o4cKKHzu6ecldFRGTCFA4lUph3eEHzDiIyDSkcSuTKBTWkk6ZJaRGZ\nlhQOJVKVTvLephrNO4jItKRwKKFrFtXx0v7j5PK6z5KITC8KhxK6ZlEdp/tytHeeKndVREQmROFQ\nQu9cDKenxYnI9KJwKKEl82ZRm0nzw+2HdZ8lEZlWFA4lZGb85vWLeerVTj7y1z/h0a1v0p/Ll7ta\nIiJjGjMczOwrZtZpZq8UldWb2ZNmtju8zy1ad7eZtZvZLjO7oah8hZm9HNZ92cwslFea2TdD+TYz\na7mwTSyvP1v7Hr71+6tpaajmL77zCnd8/flyV0lEZEzj6Tk8DKwdVHYXsMXdlwFbwmfMbDmwDrgy\n7HOfmSXDPvcDtwHLwqtwzA3AMXdfCtwLfGGyjZmqVlxWz6bfW83tH76cH+08zGFdNS0iU9yY4eDu\nPwWODiq+EdgYljcCNxWVP+buve6+F2gHVppZE1Dj7ls9en7mI4P2KRzrcWBNoVcxk5gZt7Quwh3+\n9aWD5a6OiMioJjvnMN/dC79wh4D5YbkZ2Fe03f5Q1hyWB5efs4+7Z4ETQMNwX2pmt5tZm5m1dXV1\nTbLq5bP0ktksb6rhX146UO6qiIiM6rwnpENP4KJc5eXuD7h7q7u3NjY2XoyvvOA+efUCXnjrOPuO\nnil3VURERjTZcDgchooI752hvANYVLTdwlDWEZYHl5+zj5mlgFrgyCTrNeX92i80AfBdDS2JyBQ2\n2XDYDKwPy+uBJ4rK14UzkJYQTTw/E4agus1sVZhPuHXQPoVj3Qw8FXojM9Ki+mquXVzHv/xcQ0si\nMnWN51TWbwBPA+82s/1mtgH4PPBxM9sNfCx8xt23A5uAHcAPgDvdPRcOdQfwINEk9evA90P5Q0CD\nmbUD/5Vw5tNM9qmrF7DjYLduqyEiU5ZN1z/SW1tbva2trdzVmJTO7h6u/6st/P5HruD3f+kK0skE\nFckEicSMO0lLRKYYM3vO3VvH2i51MSoj57qkpopVSxq47yevc99PXgdg4dwM//CZFVzVXFvm2omI\nKBzK5q9+/X089WonubzTl8vz9a1vcvM//Ix7b7mGT7yvqdzVE5GY07DSFNF1spffe7SN5986zh9/\n7F384ZqlzMBrAUWkzMY7rKQb700RjXMq+afbVvHr1zZz749e40/+z8/py+omfSJSHhpWmkKq0km+\neMvVXNYwi3t/9BqHTvRw/2dWUJtJl7tqIhIz6jlMMWbG5z62jC9++mqe2XuUT//Dz+jUjfpE5CJT\nOExR/3HFQjb+zkr2HzvLbzywlYMnzpa7SiISIwqHKeyDS+fxyO+spOtkL7/xj1vZf0z3YxKRi0Nn\nK00DL7x1jFu/8gyVqQTvubSGWZVJZlemqZ+Vpn5WJU21Vay96lKq0smxDyYisaaL4GaQaxfP5Ru3\nreLeJ1/j6Jk+Ok/2cKony9EzffT0R2c0feSFRv7xt1coIETkglDPYZo705fln1/o4L9/5xU+tHQe\n//vWVgWEiIxIPYeYqK5I8VvXX0Y6keDPvv0Stz3Sxm+vuoyEGcmkMbe6gktrqmicU0lS924SkXFS\nOMwQt1y3CAz+7Fsv8e+73x6yPmEwqzJFJp0kU5GkuS7DVc21XLmghuuXNHBpbVUZai0iU5XCYQa5\npXURv7hsHkdO9ZF3J5t3jp7q41B3D4e7ezjVm6WnP8fp3hxvHDnNw//vDfpyeczgupZ6Pnn1AlZf\nXk9ddQV1mTSppE5mE4krhcMM01Sboak2M65t+3N5Xjt8ki07O9n88wP8xXdeOWf9pTVVrGiZS+tl\nc1m5pJ7lTTW635NITGhCWgBwd149dJLXDp/k+Jl+jp3p4/Wu0zz3xlEOnIiu0J43u5IPv2serZfV\nU5tJh1NqU1RXpJhVmWROVZq51WkFiMgUpglpmRAz471NNby3qWbIuo7jZ3n69SP89LUufvxqJ99+\nvmOYI0Sq0gkW1GVorsvQOLuSeXMqaZhVQW0mPfBaNn8OjXMqS9kcETlPCgcZU3NdhptXLOTmFQvJ\n5Z1D3dF1Fqd6o9fZviyne3OcONvPgeNn6Th+lgPHz7Kn6zRvn+qld5i7y15aU8VVzbU0zKqgMp0g\nk07yrvlz+MDShnEPi4lI6SgcZEKSCaO5bvw/3u7O6b4oOLrP9nP0dB87D3bzcscJdhzo5pWOE/Rk\nc5zpyw3conzJvFksnJuhMpWgIpWgYVYlC+dmWDi3mtpMmkQCkmY0zK6gpWGWJs5FSkDhICVlZsyu\nTDG7MjUQKh9cOm/Idvl8NOfxs9ffZuueoxw53cuRU3l6sznePnWEE2f7hz1+ZSrBu+bPYVF9BjPD\ngOqKJIvrq1ncMIvmugxzqqLvn1WRIp0y0skEqYRpbkRkFAoHmRISCWP5ghqWL6jhd3/x8iHrT/b0\ns//YWU72ZMnlnbw7h7t72Hmwmx0Hu9l16CQO4HCyN0vXyd5Rv68ileDqhbVc11LPtYvnUledpjIV\nDW8tqq/WVeYSewoHmRbmVKV5b9P4H3p0pi/LW0fPcPB4dH3H6d4sp/tyZHN5+nN5jp3p57k3j/HA\nT/eQzZ97xp4ZLK6vZtkls7mkpoq6TJq51RVUpBIkLOoNJcxIGCTMqK5MUptJU5epoCaTYk5VmjlV\nKdIa7pJpTOEgM1J1RYr3XFrDey4devZVsTN9WV49dJIzvTl6szlO9WbZ+/Zpdh8+RXvnKV7cd5xj\nZ/rJ5Sd+ync6GQ1hFV4VSaMilSCVTAwEi5mRDHMoyYRRmUpSmU5QkUyQTBiJhA2sS4blROKdYEon\nE6RTRmUyQVVFklkVKTIVyWi+Jhl9V2UqQVU6OTCHk0oYqWSCqnRiYLgtoVuryCBTJhzMbC3wJSAJ\nPOjuny9zlSQGqitSvH/x3FG3cXdO9mbJ5qLhrHzecSCXd3J552x/juNn+gcm3U/29HOyJ8vZ/hz9\nuTz9Oac3G/VY+rJ5svk87pB3J5cvvEevvmyeo6f76Mvmo7LwfTl3crnw2aM65fJOf87pC8c9v38O\nSaorolurVKWSA2GUKrwPzNNANLMT9bCi93fCKmEMhGEqGfWwILp9SyoZBVbUA3vnGIXAK3xXwqLl\n4hCMvpfCF75THo5RqFPx20CghlC1QccpBLNZtP6dtg6dj7KwfSoRBXw6tKMQuBXhn09yBs1lTYlw\nMLMk8PfAx4H9wLNmttndd5S3ZiLRj0JN1dR+jrd7FECne7PRmV+5d8KoL5unpz9PTwirbN7J5qOy\nwinJp3uznOnPcbYvR09/bmBepz9XCKHoOF70fYXlvAMhtLJ5J5ePArEQWD6wLk9vtug4Dk60bjI9\ns6kqYQyERCHEigMwlXwnrCCEW1GeGDbwuTgYi0Pnc2uW8cmrF5S0HVMiHICVQLu77wEws8eAGwGF\ng8g4mBlV6SRV6SQN5a7MJBR6QlFPKQqSQg8pH3pZBXl3cAbKiwPrneNFgZMNIZcPx80P2ibvRb2z\nsH027+ceK/yP42Rzhd5ajv6s05vN0dOfpz+fJ5fzEI6F72SgpxkFYH5gffQd0TEpykUvaocX1XHw\njSxqM6X/Y2WqhEMzsK/o837g+jLVRUQuMrPoL+p3fpB0tli5TavTKczsdjNrM7O2rq6ucldHRGTG\nmirh0AEsKvq8MJSdw90fcPdWd29tbGy8aJUTEYmbqRIOzwLLzGyJmVUA64DNZa6TiEhsTYk5B3fP\nmtkfAD8kGmz8irtvL3O1RERia0qEA4C7fw/4XrnrISIiU2dYSUREphCFg4iIDKFwEBGRIabtM6TN\nrAt4c5K7zwPevoDVmS7i2O44thni2e44thkm3u7L3H3MawGmbTicDzNrG88DtmeaOLY7jm2GeLY7\njm2G0rVbw0oiIjKEwkFERIaIazg8UO4KlEkc2x3HNkM82x3HNkOJ2h3LOQcRERldXHsOIiIyitiF\ng5mtNbNdZtZuZneVuz6lYGaLzOzHZrbDzLab2edCeb2ZPWlmu8P76M/HnIbMLGlmL5jZd8PnOLS5\nzsweN7NXzWynma2e6e02sz8O/22/YmbfMLOqmdhmM/uKmXWa2StFZSO208zuDr9tu8zshvP57liF\nQ9HjSD8BLAf+k5ktL2+tSiIL/Im7LwdWAXeGdt4FbHH3ZcCW8Hmm+Ryws+hzHNr8JeAH7v4e4Gqi\n9s/YdptZM/CHQKu7X0V0s851zMw2PwysHVQ2bDvD/8fXAVeGfe4Lv3mTEqtwoOhxpO7eBxQeRzqj\nuPtBd38+LJ8k+rFoJmrrxrDZRuCm8tSwNMxsIfCrwINFxTO9zbXAh4GHANy9z92PM8PbTXTT0IyZ\npYBq4AAzsM3u/lPg6KDikdp5I/CYu/e6+16gneg3b1LiFg7DPY60uUx1uSjMrAW4FtgGzHf3g2HV\nIWB+mapVKn8H/CmQLyqb6W1eAnQBXw3DaQ+a2SxmcLvdvQP4G+At4CBwwt3/LzO4zYOM1M4L+vsW\nt3CIFTObDXwL+CN37y5e59FpajPmVDUz+zWg092fG2mbmdbmIAW8H7jf3a8FTjNoOGWmtTuMsd9I\nFIwLgFlm9pnibWZam0dSynbGLRzG9TjSmcDM0kTB8HV3/3YoPmxmTWF9E9BZrvqVwAeBT5nZG0TD\nhR81s69dez/7AAABLklEQVQxs9sM0V+H+919W/j8OFFYzOR2fwzY6+5d7t4PfBv4ADO7zcVGaucF\n/X2LWzjE4nGkZmZEY9A73f1vi1ZtBtaH5fXAExe7bqXi7ne7+0J3byH69/qUu3+GGdxmAHc/BOwz\ns3eHojXADmZ2u98CVplZdfhvfQ3RvNpMbnOxkdq5GVhnZpVmtgRYBjwz6W9x91i9gF8BXgNeB/68\n3PUpURs/RNTVfAl4Mbx+BWggOrthN/AjoL7cdS1R+38J+G5YnvFtBq4B2sK/7+8Ac2d6u4G/BF4F\nXgEeBSpnYpuBbxDNq/QT9RI3jNZO4M/Db9su4BPn8926QlpERIaI27CSiIiMg8JBRESGUDiIiMgQ\nCgcRERlC4SAiIkMoHEREZAiFg4iIDKFwEBGRIf4/rhXWY9HCHgMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23b80ce05f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: 统计一下qlist中每个单词出现的频率，并把这些频率排一下序，然后画成plot. 比如总共出现了总共7个不同单词，而且每个单词出现的频率为 4, 5,10,2, 1, 1,1\n",
    "#       把频率排序之后就可以得到(从大到小) 10, 5, 4, 2, 1, 1, 1. 然后把这7个数plot即可（从大到小）\n",
    "#       需要使用matplotlib里的plot函数。y轴是词频\n",
    "%matplotlib inline\n",
    "\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "qs_counter = collections.Counter(q_total)\n",
    "qs, qs_c = zip(*qs_counter.most_common())\n",
    "plt.plot(range(100), qs_c[:100]);\n",
    "# plt.plot(range(len(qs_c)), qs_c);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO： 从上面的图中能观察到什么样的现象？ 这样的一个图的形状跟一个非常著名的函数形状很类似，能所出此定理吗？ \n",
    "#       hint: [XXX]'s law\n",
    "# 指数函数\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qlist出现次数最多的TOP 10单词及词频:\n",
      " [('the', 60959), ('What', 36994), ('of', 33300), ('', 29438), ('in', 21267), ('to', 17579), ('was', 17041), ('is', 16165), ('did', 15624), ('what', 11255)]\n",
      "alist出现次数最多的TOP 10单词及词频:\n",
      " [('the', 13211), ('of', 8892), ('and', 8172), ('to', 3440), ('a', 3294), ('in', 2838), ('The', 1769), ('or', 1261), ('for', 1032), ('million', 853)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: 在qlist和alist里出现次数最多的TOP 10单词分别是什么？ \n",
    "a_total = [word for each_a in alist for word in each_a.split(' ') ]\n",
    "a_total_set = set(a_total)\n",
    "as_counter = collections.Counter(a_total)\n",
    "print(\"qlist出现次数最多的TOP 10单词及词频:\\n\", qs_counter.most_common(10))\n",
    "print(\"alist出现次数最多的TOP 10单词及词频:\\n\", as_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 文本预处理\n",
    "次部分需要尝试做文本的处理。在这里我们面对的是英文文本，所以任何对英文适合的技术都可以考虑进来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "class DictTree(object):\n",
    "    def __init__(self, name='root', word=None, info=None):\n",
    "        self.name = name\n",
    "        self.info = info\n",
    "        self.word = word\n",
    "        self.children = {}\n",
    "    \n",
    "    def __str__(self):\n",
    "        return json.dumps({self.word: self.info}, ensure_ascii=False)\n",
    "    \n",
    "    def set_info(self, info):\n",
    "        self.info = info\n",
    "    \n",
    "    def find_child(self, name):\n",
    "        \"\"\"查询子节点是否存在\"\"\"\n",
    "        if name in self.children.keys():\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def add_word(self, chars, info, pointer=0):\n",
    "        \"\"\"将数据创建成字典树\"\"\"\n",
    "        if len(chars[pointer:]) <= 0:\n",
    "            self.info = info\n",
    "            self.word = chars\n",
    "        else:\n",
    "            if not self.find_child(chars[pointer]):\n",
    "                self.children[chars[pointer]] = DictTree(chars[pointer])\n",
    "            self.children[chars[pointer]].add_word(chars, info, pointer + 1)\n",
    "    \n",
    "    def add_words(self, words, infos):\n",
    "        \"\"\"添加多组词\"\"\"\n",
    "        for word, info in zip(words, infos):\n",
    "            self.add_word(word, info)\n",
    "            \n",
    "    def add_words2(self, words):\n",
    "        \"\"\"添加多组词，不添加此相关信息\"\"\"\n",
    "        for word in words:\n",
    "            self.add_word(word, None)\n",
    "    \n",
    "    def cut_word(self, chars):\n",
    "        \"\"\"\n",
    "        查询字典树中是否有某个词\n",
    "        return: 没有该词会返回空，有该值会返回该词的信息\n",
    "        \"\"\"\n",
    "        if len(chars) == 0:\n",
    "            if self.word is None:\n",
    "                return None\n",
    "            else:\n",
    "                return self\n",
    "        elif self.find_child(chars[0]):\n",
    "            return self.children[chars[0]].cut_word(chars[1:])\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def _cut_words(self, chars):\n",
    "        \"\"\"\n",
    "        查询字典树中是否有某个词\n",
    "        return: 没有该词会返回空，有该值会返回该词的信息\n",
    "        \"\"\"\n",
    "        words = set()\n",
    "        if len(chars) == 0:\n",
    "            return words\n",
    "        if self.word is not None:\n",
    "            words.add(self)\n",
    "        if self.find_child(chars[0]):\n",
    "            words.update(self.children[chars[0]]._cut_words(chars[1:]))\n",
    "        return words\n",
    "        \n",
    "    def cut_words(self, sentence):\n",
    "        \"\"\"查询字典树\"\"\"\n",
    "        words = set()\n",
    "        if len(sentence) == 0:\n",
    "            return words\n",
    "        for n in range(len(sentence)):\n",
    "            words.update(self._cut_words(sentence[n:]))\n",
    "        return words\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"输出成dict\"\"\"\n",
    "        children = {}\n",
    "        for child_name, child_node in self.children.items():\n",
    "            children[child_name] = child_node.to_dict()\n",
    "        return {\n",
    "            'name': self.name,\n",
    "            'info': self.info,\n",
    "            'children': children,\n",
    "        }\n",
    "    \n",
    "    def read_dict(self, node_dict):\n",
    "        \"\"\"从dict中读入\"\"\"\n",
    "        self.name = node_dict['name']\n",
    "        self.info = node_dict['info']\n",
    "        for child_name, child_dict in node_dict['children'].items():\n",
    "            self.children[child_name] = Node(None).read_dict(child_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/英文停用词.txt', 'r', encoding='utf-8') as f:\n",
    "    words = set(f.read().split('\\n') + [''])\n",
    "stop_words = DictTree()\n",
    "stop_words.add_words2(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "def cut(sentence):\n",
    "    \"\"\"英文分词\"\"\"\n",
    "    words = []\n",
    "    for word in re.split(r'\\s+', sentence):\n",
    "        find_punc = re.search('^(.+)([,.?!:;]+)$', word)\n",
    "        if find_punc is not None:\n",
    "            words.append(find_punc.groups()[0])\n",
    "            words.append(find_punc.groups()[1])\n",
    "        else:\n",
    "            words.append(word)\n",
    "    return words\n",
    "\n",
    "def lower_case(words):\n",
    "    \"\"\"全部转为小写字母\"\"\"\n",
    "    return [i.lower() for i in words]\n",
    "\n",
    "def del_stop_words(words):\n",
    "    \"\"\"去除停用词\"\"\"\n",
    "    # 不在停用词表的词\n",
    "    return [i for i in words if stop_words.cut_word(i) is None]\n",
    "\n",
    "def porter_stemming(words):\n",
    "    return [porter_stemmer.stem(w) for w in words]\n",
    "\n",
    "def number2word(words):\n",
    "    return ['#number' if re.search(r'\\d+\\.*\\d*', word) else word for word in words]\n",
    "\n",
    "def get_counter(all_words):\n",
    "    \"\"\"获取词频\"\"\"\n",
    "    return collections.Counter([word for words in all_words for word in words])\n",
    "\n",
    "def del_low_freq_words(all_words, w_counter, low_freq=1):\n",
    "    \"\"\"删除低频词\n",
    "    param: all_words: [[words], ...]\n",
    "    param: low_freq: 删除低于该频率的词\n",
    "    \"\"\"\n",
    "    low_freq_words = [i[0] for i in w_counter.most_common() if i[1] <= low_freq]\n",
    "    lfw_tree = DictTree()\n",
    "    lfw_tree.add_words2(low_freq_words)\n",
    "    print(len(low_freq_words), len(w_counter.most_common()))\n",
    "    return [[word for word in words if lfw_tree.cut_word(word) is None] for words in all_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'bjm', '.', '']\n",
      "['I', 'bjm', '.']\n",
      "2 6\n",
      "[['1', '1', '3', '3'], ['1', '3', '4', '4', '5', '5']]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(cut('I am bjm. '))\n",
    "print(del_stop_words(cut('I am bjm. ')))\n",
    "test_all_words = [['1','1','2','3','3'],['1','3','4','4','5','5', '8']]\n",
    "print(del_low_freq_words(test_all_words, get_counter(test_all_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34927 40382\n",
      "37235 40093\n"
     ]
    }
   ],
   "source": [
    "def sentence2words(sentence):\n",
    "    \"\"\"单个句子的预处理，处理为词\"\"\"\n",
    "    words = cut(sentence)\n",
    "    dsw_words = del_stop_words(words)\n",
    "    # ps_words = porter_stemming(dsw_words)\n",
    "    lc_words = lower_case(dsw_words)\n",
    "    dsw_words = del_stop_words(lc_words)\n",
    "    n2w_words = number2word(dsw_words)\n",
    "    return n2w_words\n",
    "\n",
    "def preprocess(sentences):\n",
    "    \"\"\"数据处理\"\"\"\n",
    "    all_words = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence2words(sentence)\n",
    "        all_words.append(words)\n",
    "    w_counter = get_counter(all_words)\n",
    "    all_words = del_low_freq_words(all_words, w_counter, low_freq=10)\n",
    "    return all_words\n",
    "all_q_words = preprocess(qlist)\n",
    "all_a_words = preprocess(alist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When did Beyonce start becoming popular?\n",
      "['beyonce', 'start', 'popular', '?']\n",
      "What areas did Beyonce compete in when she was growing up?\n",
      "['beyonce', 'compete', 'growing', '?']\n",
      "When did Beyonce leave Destiny's Child and become a solo singer?\n",
      "['beyonce', 'leave', \"destiny's\", 'child', 'solo', 'singer', '?']\n",
      "In what city and state did Beyonce  grow up? \n",
      "['city', 'beyonce', 'grow', '?']\n",
      "In which decade did Beyonce become famous?\n",
      "['decade', 'beyonce', 'famous', '?']\n",
      "In what R&B group was she the lead singer?\n",
      "['r&b', 'lead', 'singer', '?']\n",
      "What album made her a worldwide known artist?\n",
      "['album', 'worldwide', 'artist', '?']\n",
      "Who managed the Destiny's Child group?\n",
      "['managed', \"destiny's\", 'child', '?']\n",
      "When did Beyoncé rise to fame?\n",
      "['beyoncé', 'rise', 'fame', '?']\n",
      "What role did Beyoncé have in Destiny's Child?\n",
      "['role', 'beyoncé', \"destiny's\", 'child', '?']\n"
     ]
    }
   ],
   "source": [
    "# 打印前十个问题，及其分词\n",
    "for i, j in zip(qlist, all_q_words[:10]):\n",
    "    print('%s\\n%s' % (i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 若返回空则证明数字已被处理为#number\n",
    "i = 0\n",
    "for q_words in all_q_words:\n",
    "    for word in q_words:\n",
    "        if re.search(r'^\\d+$', word) is not None:\n",
    "            i += 1\n",
    "            if i >= 10:\n",
    "                break\n",
    "            print(word, q_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: 在前面步骤里，我们删除了出现次数比较少的单词，那你选择的阈值是多少（小于多少的去掉？）， 这个阈值是根据什么来选择的？ \n",
    "#  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 文本表示\n",
    "当我们做完关键的预处理过程之后，就需要把每一个文本转换成向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分数（10）\n",
    "\n",
    "# TODO: 把qlist中的每一个问题字符串转换成tf-idf向量, \n",
    "# 转换之后的结果存储在X矩阵里。 X的大小是： N* D的矩阵。 \n",
    "# 这里N是问题的个数（样本个数），\n",
    "#       D是字典库的大小。 \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "q_corpus = [' '.join(q_words) for q_words in all_q_words]\n",
    "\n",
    "# 直接生成tf-idf值\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit_transform(q_corpus).todense()\n",
    "\n",
    "# tfidf_matrix = tfidf_vec.fit_transform(q_corpus).todense()\n",
    "# vectorizer =  # 定义一个tf-idf的vectorizer\n",
    "\n",
    "X = vectorizer.transform(q_corpus)  # 结果存放在X矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4572)\t0.559309159306\n",
      "  (0, 3642)\t0.582243424418\n",
      "  (0, 498)\t0.590055810105\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# X_2 = [[[n, j] for n, j in enumerate(i) if j != 0] for i in tfidf_matrix.toarray()]\n",
    "# print(X_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 矩阵X有什么特点？ 计算一下它的稀疏度\n",
    "# sparsity = len(X[X[i]!=0, i])/len(X[i])\n",
    "# print (sparsity)  # 打印出稀疏度(sparsity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 对于用户的输入问题，找到相似度最高的TOP5问题，并把5个潜在的答案做返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4190)\t0.391256639144\n",
      "  (0, 2449)\t0.554388627511\n",
      "  (0, 1389)\t0.488485807568\n",
      "  (0, 814)\t0.38588564231\n",
      "  (0, 499)\t0.389929966629\n"
     ]
    }
   ],
   "source": [
    "# 分数（10）\n",
    "\n",
    "def top5results(input_q):\n",
    "    \"\"\"\n",
    "    给定用户输入的问题 input_q, 返回最有可能的TOP 5问题。这里面需要做到以下几点：\n",
    "    1. 对于用户的输入 input_q 首先做一系列的预处理，然后再转换成tf-idf向量（利用上面的vectorizer)\n",
    "    2. 计算跟每个库里的问题之间的相似度\n",
    "    3. 找出相似度最高的top5问题的答案\n",
    "    \"\"\"\n",
    "#     tfidf_matrix.todense()\n",
    "    tfidf_q = vectorizer.transform([input_q])\n",
    "    print(tfidf_q)\n",
    "    top_idxs = []  # top_idxs存放相似度最高的（存在qlist里的）问题的下表 \n",
    "                   # hint: 利用priority queue来找出top results. 思考为什么可以这么做？ \n",
    "    \n",
    "#     return alist[top_idxs]  # 返回相似度最高的问题对应的答案，作为TOP5答案    \n",
    "top5results(\"What role did Beyoncé have in Destiny's Child?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1.0\n",
      "  (1, 2)\t1.0\n",
      "  (2, 0)\t1.0\n",
      "  (0, 2)\t1.0\n",
      "  (2, 0)\t1.0\n"
     ]
    }
   ],
   "source": [
    "X_train = ['hello', 'hi', 'hei']\n",
    "X_test = ['hi', 'haa', 'hei']\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "vectorizer.fit_transform(X_train).todense()\n",
    "X_train = vectorizer.transform(X_train)\n",
    "print(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
